#!/usr/bin/env python
# -*- coding: utf-8 -*-
!pip install colorama requests tqdm matplotlib pandas psutil gradio
"""
DeepSeek Models with Ollama on Google Colab
===================================================
This script sets up and runs DeepSeek models using Ollama in a Google Colab environment
with an enhanced, visually appealing console UI for a better chat experience.

Features:
- Beautiful color-themed console interface
- Rich message styling and visual feedback
- Animated responses with typing effect
- Enhanced menu systems and data visualization
- All original DeepSeek functionality preserved
- Improved user experience for chat and commands

Date: April 15, 2025
"""

import os
import time
import requests
import subprocess
import IPython.display
from IPython.display import HTML, display, clear_output
import json
import sys
import signal
import datetime
import psutil
import threading
import re
import glob
import csv
import matplotlib.pyplot as plt
import pandas as pd
from tqdm.auto import tqdm
import shutil

# Try importing optional libraries
try:
    import gradio as gr
    GRADIO_AVAILABLE = True
except ImportError:
    GRADIO_AVAILABLE = False

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import colorama
    from colorama import Fore, Back, Style
    COLORAMA_AVAILABLE = True
except ImportError:
    COLORAMA_AVAILABLE = False

# Display styled header
display(HTML("""
<div style="background-color: #f8f9fa; padding: 10px; border-radius: 10px; margin-bottom: 15px;">
    <h1 style="text-align: center; color: #4285f4;">Enhanced DeepSeek Models with Ollama on Google Colab</h1>
    <p style="text-align: center; font-style: italic;">Set up and run DeepSeek models locally using Ollama with a beautiful console interface</p>
</div>
"""))

# Check if running in Google Colab
try:
    import google.colab
    IN_COLAB = True
    print("✅ Running in Google Colab environment")
except ImportError:
    IN_COLAB = False
    print("⚠️ Not running in Google Colab environment. Some features might not work correctly.")

#===============================================================================
# ConsoleUI Class - Enhanced Terminal Interface
#===============================================================================

class ConsoleUI:
    """Enhanced Console UI class for beautiful terminal interactions"""
    
    def __init__(self, app_name="DeepSeek Chat", theme="blue"):
        """Initialize the console UI with theme settings"""
        self.app_name = app_name
        
         # Check if colorama is available, if not try to install it
        global COLORAMA_AVAILABLE
        if not COLORAMA_AVAILABLE:
            try:
                if IN_COLAB:
                    subprocess.run(["pip", "install", "-q", "colorama"], check=True)
                    import colorama
                    from colorama import Fore, Back, Style
                    COLORAMA_AVAILABLE = True
                    colorama.init()
                    print("✅ Installed colorama for terminal colors")
                else:
                    print("⚠️ Failed to install colorama. Using fallback styling.")
            except:
                print("⚠️ Failed to install colorama. Using fallback styling.")
        else:
            # Need to import these again in this scope
            import colorama
            from colorama import Fore, Back, Style
            colorama.init()
        
        # Set terminal width
        try:
            self.terminal_width = shutil.get_terminal_size().columns
        except:
            self.terminal_width = 80  # Fallback width
            
        self.chat_history = []
        self.set_theme(theme)
        
    def set_theme(self, theme_name):
        """Set color theme for the UI"""
        # Use colorama if available, otherwise use fallback
        if COLORAMA_AVAILABLE:
            themes = {
                "blue": {
                    "primary": Fore.BLUE,
                    "secondary": Fore.CYAN,
                    "accent": Fore.LIGHTBLUE_EX,
                    "success": Fore.GREEN,
                    "error": Fore.RED,
                    "warning": Fore.YELLOW,
                    "user": Fore.WHITE,
                    "ai": Fore.CYAN,
                    "system": Fore.MAGENTA,
                    "header_bg": Back.BLUE,
                    "header_fg": Fore.WHITE,
                    "reset": Style.RESET_ALL
                },
                "green": {
                    "primary": Fore.GREEN,
                    "secondary": Fore.LIGHTGREEN_EX,
                    "accent": Fore.CYAN,
                    "success": Fore.GREEN,
                    "error": Fore.RED,
                    "warning": Fore.YELLOW,
                    "user": Fore.WHITE, 
                    "ai": Fore.LIGHTGREEN_EX,
                    "system": Fore.MAGENTA,
                    "header_bg": Back.GREEN,
                    "header_fg": Fore.BLACK,
                    "reset": Style.RESET_ALL
                },
                "purple": {
                    "primary": Fore.MAGENTA,
                    "secondary": Fore.LIGHTMAGENTA_EX,
                    "accent": Fore.CYAN, 
                    "success": Fore.GREEN,
                    "error": Fore.RED,
                    "warning": Fore.YELLOW,
                    "user": Fore.WHITE,
                    "ai": Fore.LIGHTMAGENTA_EX,
                    "system": Fore.BLUE,
                    "header_bg": Back.MAGENTA,
                    "header_fg": Fore.WHITE,
                    "reset": Style.RESET_ALL
                },
                "dark": {
                    "primary": Fore.WHITE,
                    "secondary": Fore.LIGHTBLACK_EX, 
                    "accent": Fore.CYAN,
                    "success": Fore.LIGHTGREEN_EX,
                    "error": Fore.LIGHTRED_EX,
                    "warning": Fore.LIGHTYELLOW_EX,
                    "user": Fore.LIGHTCYAN_EX,
                    "ai": Fore.LIGHTWHITE_EX,
                    "system": Fore.LIGHTMAGENTA_EX,
                    "header_bg": Back.BLACK,
                    "header_fg": Fore.WHITE,
                    "reset": Style.RESET_ALL
                },
            }
        else:
            # Fallback theme without colorama (just empty strings)
            themes = {
                "blue": {
                    "primary": "",
                    "secondary": "",
                    "accent": "",
                    "success": "",
                    "error": "",
                    "warning": "",
                    "user": "",
                    "ai": "",
                    "system": "",
                    "header_bg": "",
                    "header_fg": "",
                    "reset": ""
                }
            }
            # Use same fallback for all themes
            themes["green"] = themes["blue"]
            themes["purple"] = themes["blue"]
            themes["dark"] = themes["blue"]
        
        self.theme = themes.get(theme_name, themes["blue"])
    
    def clear(self):
        """Clear the console"""
        clear_output(wait=True)
        
    def display_header(self):
        """Display a styled application header"""
        width = self.terminal_width
        header = f" {self.app_name} "
        padding = "=" * ((width - len(header)) // 2)
        
        print(f"\n{self.theme['header_bg']}{self.theme['header_fg']}{padding}{header}{padding}{self.theme['reset']}")
        
    def display_splash(self):
        """Display a fancy splash screen"""
        self.clear()
        
        logo = f"""
{self.theme['primary']}        _____                 _____           _     
       |  __ \\               / ____|         | |    
       | |  | | ___  ___ _ _| (___   ___  ___| | __ 
       | |  | |/ _ \\/ _ \\ '_ \\___ \\ / _ \\/ _ \\ |/ / 
       | |__| |  __/  __/ |_) |___) |  __/  __/   <  
       |_____/ \\___|\\___| .__/_____/ \\___|\\___|_|\\_\\
                        | |                         
                        |_|                         {self.theme['reset']}
        """
        
        print(logo)
        print(f"{self.theme['secondary']}═══════════════════════════════════════════════════════════════{self.theme['reset']}")
        print(f"{self.theme['accent']}        Enhanced Chat Experience with Ollama{self.theme['reset']}")
        print(f"{self.theme['secondary']}═══════════════════════════════════════════════════════════════{self.theme['reset']}")
        
    def print_section_header(self, title):
        """Print a styled section header"""
        width = self.terminal_width
        padding_char = "─"
        padding = padding_char * 3
        
        print(f"\n{self.theme['primary']}{padding} {title} {padding_char * (width - len(title) - len(padding) - 5)}{self.theme['reset']}")
    
    def print_success(self, message):
        """Print a success message"""
        print(f"{self.theme['success']}✓ {message}{self.theme['reset']}")
        
    def print_error(self, message):
        """Print an error message"""
        print(f"{self.theme['error']}✗ {message}{self.theme['reset']}")
        
    def print_warning(self, message):
        """Print a warning message"""
        print(f"{self.theme['warning']}⚠ {message}{self.theme['reset']}")
        
    def print_info(self, message):
        """Print an info message"""
        print(f"{self.theme['accent']}ℹ {message}{self.theme['reset']}")
    
    def print_menu(self, options):
        """Display a styled menu with options"""
        self.print_section_header("Menu")
        
        for idx, option in enumerate(options, 1):
            print(f"  {self.theme['primary']}{idx}.{self.theme['reset']} {option}")
        
        choice = input(f"\n{self.theme['secondary']}Enter your choice: {self.theme['reset']}")
        return choice
    
    def prompt_input(self, prompt_text, multiline=False):
        """Get user input with a styled prompt"""
        if multiline:
            print(f"{self.theme['secondary']}{prompt_text} (Enter empty line to finish):{self.theme['reset']}")
            lines = []
            while True:
                line = input(f"{self.theme['secondary']}... {self.theme['reset']}")
                if not line:
                    break
                lines.append(line)
            return "\n".join(lines)
        else:
            return input(f"{self.theme['secondary']}{prompt_text}: {self.theme['reset']}")
    
    def display_progress(self, percentage, prefix="", suffix="", bar_length=30):
        """Display a styled progress bar"""
        filled_length = int(bar_length * percentage // 100)
        bar = '█' * filled_length + '░' * (bar_length - filled_length)
        print(f"\r{self.theme['secondary']}{prefix} |{self.theme['primary']}{bar}{self.theme['reset']}| {percentage:.1f}% {suffix}", end="\r")
        if percentage == 100:
            print()
    
    def display_chat_message(self, role, content, model_name=None):
        """Display a styled chat message"""
        width = self.terminal_width
        timestamp = time.strftime("%H:%M:%S")
        
        # Add to chat history
        self.chat_history.append({
            "role": role,
            "content": content,
            "timestamp": timestamp
        })
        
        # Display with appropriate styling
        if role.lower() == "user":
            # User message styling
            print(f"\n{self.theme['user']}╭──────── You ({timestamp}) ────────{self.theme['reset']}")
            print(f"{self.theme['user']}│ {content}{self.theme['reset']}")
            print(f"{self.theme['user']}╰{'─' * (width - 2)}{self.theme['reset']}")
            
        elif role.lower() == "assistant":
            # Assistant message styling
            model_info = f" [{model_name}]" if model_name else ""
            print(f"\n{self.theme['ai']}╭──────── AI{model_info} ({timestamp}) ────────{self.theme['reset']}")
            
            # Handle code blocks in AI response
            lines = content.split('\n')
            in_code_block = False
            
            for line in lines:
                if '```' in line and not in_code_block:
                    parts = line.split('```', 1)
                    print(f"{self.theme['ai']}│ {parts[0]}{self.theme['reset']}", end="")
                    if len(parts) > 1 and parts[1]:
                        print(f"{self.theme['accent']}```{parts[1]}{self.theme['reset']}")
                    else:
                        print(f"{self.theme['accent']}```{self.theme['reset']}")
                    in_code_block = True
                    continue
                    
                elif '```' in line and in_code_block:
                    parts = line.split('```', 1)
                    if parts[0]:
                        print(f"{self.theme['accent']}{parts[0]}{self.theme['reset']}", end="")
                    print(f"{self.theme['accent']}```{self.theme['reset']}", end="")
                    if len(parts) > 1 and parts[1]:
                        print(f"{self.theme['ai']}{parts[1]}{self.theme['reset']}")
                    else:
                        print()
                    in_code_block = False
                    continue
                    
                elif in_code_block:
                    print(f"{self.theme['accent']}{line}{self.theme['reset']}")
                else:
                    print(f"{self.theme['ai']}│ {line}{self.theme['reset']}")
            
            print(f"{self.theme['ai']}╰{'─' * (width - 2)}{self.theme['reset']}")
            
        elif role.lower() == "system":
            # System message styling
            print(f"\n{self.theme['system']}┌──────── System ────────{self.theme['reset']}")
            print(f"{self.theme['system']}│ {content}{self.theme['reset']}")
            print(f"{self.theme['system']}└{'─' * (width - 2)}{self.theme['reset']}")
    
    def display_thinking(self, model_name=None):
        """Display a 'thinking' animation while the model is generating a response"""
        print(f"\n{self.theme['ai']}╭──────── AI{' ['+model_name+']' if model_name else ''} ────────{self.theme['reset']}")
        
        thinking_frames = ["⣾", "⣽", "⣻", "⢿", "⡿", "⣟", "⣯", "⣷"]
        
        print(f"{self.theme['ai']}│ Thinking", end="", flush=True)
        for _ in range(3):  # Loop through animation frames
            for frame in thinking_frames:
                print(f"\r{self.theme['ai']}│ Thinking {frame}", end="", flush=True)
                time.sleep(0.1)
        
        print(f"\r{self.theme['ai']}│ ", end="", flush=True)
    
    def live_response(self, text, model_name=None):
        """Display the model's response character by character for a typing effect"""
        width = self.terminal_width
        timestamp = time.strftime("%H:%M:%S")
        
        # Display header
        model_info = f" [{model_name}]" if model_name else ""
        print(f"\n{self.theme['ai']}╭──────── AI{model_info} ({timestamp}) ────────{self.theme['reset']}")
        
        # Type characters with a realistic delay
        print(f"{self.theme['ai']}│ ", end="", flush=True)
        
        current_line_length = 0
        in_code_block = False
        
        lines = text.split('\n')
        for i, line in enumerate(lines):
            # Special handling for code blocks
            if '```' in line:
                if not in_code_block:
                    parts = line.split('```', 1)
                    for char in parts[0]:
                        print(char, end="", flush=True)
                        time.sleep(0.01)
                    print(f"{self.theme['accent']}```{parts[1] if len(parts) > 1 else ''}{self.theme['reset']}")
                    in_code_block = True
                else:
                    parts = line.split('```', 1)
                    if parts[0]:
                        print(f"{self.theme['accent']}{parts[0]}", end="", flush=True)
                    print(f"{self.theme['accent']}```{self.theme['reset']}", end="")
                    if len(parts) > 1:
                        for char in parts[1]:
                            print(char, end="", flush=True)
                            time.sleep(0.01)
                    print()
                    in_code_block = False
            elif in_code_block:
                print(f"{self.theme['accent']}{line}{self.theme['reset']}")
                time.sleep(0.02)
            else:
                # Regular text typing effect
                for char in line:
                    print(char, end="", flush=True)
                    
                    # Adjust typing speed based on characters
                    if char in '.!?':
                        time.sleep(0.05)  # Longer pause after sentence endings
                    elif char == ',':
                        time.sleep(0.03)  # Brief pause after commas
                    else:
                        time.sleep(0.01)  # Default typing speed
                
                # New line if not the last line
                if i < len(lines) - 1:
                    print()
                    print(f"{self.theme['ai']}│ ", end="", flush=True)
        
        print(f"\n{self.theme['ai']}╰{'─' * (width - 2)}{self.theme['reset']}")
        
        # Add to chat history
        self.chat_history.append({
            "role": "assistant",
            "content": text,
            "timestamp": timestamp
        })
    
    def save_chat_history(self, filename=None):
        """Save the chat history to a file"""
        if not filename:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"chat_history_{timestamp}.json"
        
        try:
            # Ensure directory exists
            if not os.path.exists("./chat_history"):
                os.makedirs("./chat_history")
                
            with open(f"./chat_history/{filename}", 'w', encoding='utf-8') as f:
                json.dump(self.chat_history, f, indent=2)
            self.print_success(f"Chat history saved to: ./chat_history/{filename}")
            return filename
        except Exception as e:
            self.print_error(f"Failed to save chat history: {e}")
            return None
    
    def display_help(self, commands):
        """Display available commands in a styled format"""
        self.print_section_header("Available Commands")
        
        for cmd, desc in commands.items():
            print(f"  {self.theme['primary']}{cmd}{self.theme['reset']}: {desc}")
    
    def create_table(self, headers, rows):
        """Display data in a nicely formatted table"""
        # Calculate column widths
        col_widths = [len(h) for h in headers]
        for row in rows:
            for i, cell in enumerate(row):
                col_widths[i] = max(col_widths[i], len(str(cell)))
        
        # Add padding to column widths
        col_widths = [w + 2 for w in col_widths]
        
        # Calculate total width including borders
        total_width = sum(col_widths) + len(col_widths) + 1
        
        # Draw top border
        print(f"{self.theme['secondary']}┌{'┬'.join(['─' * width for width in col_widths])}┐{self.theme['reset']}")
        
        # Draw header row
        header_str = "│"
        for i, header in enumerate(headers):
            header_str += f" {self.theme['primary']}{header}{' ' * (col_widths[i] - len(header) - 1)}{self.theme['reset']}│"
        print(header_str)
        
        # Draw header/data separator
        print(f"{self.theme['secondary']}├{'┼'.join(['─' * width for width in col_widths])}┤{self.theme['reset']}")
        
        # Draw data rows
        for row in rows:
            row_str = "│"
            for i, cell in enumerate(row):
                cell_str = str(cell)
                row_str += f" {cell_str}{' ' * (col_widths[i] - len(cell_str) - 1)}│"
            print(row_str)
        
        # Draw bottom border
        print(f"{self.theme['secondary']}└{'┴'.join(['─' * width for width in col_widths])}┘{self.theme['reset']}")
    
    def prompt_choice(self, prompt_text, options):
        """Show a list of options and get user's selection"""
        self.print_section_header(prompt_text)
        
        for idx, option in enumerate(options, 1):
            print(f"  {self.theme['primary']}{idx}.{self.theme['reset']} {option}")
        
        while True:
            choice = input(f"\n{self.theme['secondary']}Enter your choice (1-{len(options)}): {self.theme['reset']}")
            try:
                choice_idx = int(choice)
                if 1 <= choice_idx <= len(options):
                    return choice_idx, options[choice_idx-1]
                else:
                    self.print_error(f"Please enter a number between 1 and {len(options)}")
            except ValueError:
                self.print_error("Please enter a valid number")
    
    def show_web_interface(self, url):
        """Display information about accessing the web interface"""
        self.print_section_header("Web Interface")
        
        print(f"""
{self.theme['success']}✓ Web interface is now running!{self.theme['reset']}

Access the chat interface through your browser:
{self.theme['accent']}URL: {url}{self.theme['reset']}

{self.theme['warning']}Note: Keep this tab open while using the web interface.{self.theme['reset']}
Press Ctrl+C to stop the server when you're done.
""")

#===============================================================================
# Original DeepSeek Functions with UI Enhancements
#===============================================================================

# Create directories for saving data
def setup_directories():
    """Create necessary directories for saving chat history, models info, etc."""
    dirs = ['./chat_history', './benchmarks', './model_configs', './input_files']
    for d in dirs:
        if not os.path.exists(d):
            os.makedirs(d)
            print(f"📁 Created directory: {d}")

# Install Ollama
def install_ollama(ui=None):
    """Download and install Ollama, then start the server."""
    if ui:
        ui.print_section_header("Setting up Ollama")
    else:
        print("\n🚀 Setting up Ollama...")
    
    # Install necessary dependencies
    if IN_COLAB:
        try:
            !pip install -q psutil matplotlib pandas tqdm colorama
            if ui:
                ui.print_success("Installed supporting libraries")
            else:
                print("✅ Installed supporting libraries")
            
            # Optional installs
            try:
                !pip install -q gradio
                if ui:
                    ui.print_success("Installed Gradio for web UI")
                else:
                    print("✅ Installed Gradio for web UI")
            except:
                if ui:
                    ui.print_warning("Failed to install Gradio. Web UI will not be available.")
                else:
                    print("⚠️ Failed to install Gradio. Web UI will not be available.")
        except:
            if ui:
                ui.print_warning("Failed to install some supporting libraries. Some features may not work.")
            else:
                print("⚠️ Failed to install some supporting libraries. Some features may not work.")
    
    # Download and install Ollama
    if ui:
        ui.print_info("Installing Ollama...")
    !curl -fsSL https://ollama.com/install.sh | sh
    
    # Start Ollama server
    if ui:
        ui.print_info("Starting Ollama server...")
    !ollama serve > /dev/null 2>&1 &
    
    # Wait for Ollama to start
    time.sleep(5)
    
    # Ensure Ollama is running
    try:
        response = requests.get("http://localhost:11434/api/version")
        if response.status_code == 200:
            version = response.json().get("version", "unknown")
            if ui:
                ui.print_success(f"Ollama {version} is running successfully!")
            else:
                print(f"✅ Ollama {version} is running successfully!")
            return True
    except:
        if ui:
            ui.print_error("Failed to start Ollama server.")
        else:
            print("❌ Failed to start Ollama server.")
        return False

# List available models
def list_available_models(ui=None, keyword=None):
    """List available models in Ollama, with optional filtering by keyword."""
    if ui:
        ui.print_section_header("Available Models")
    else:
        print("\n📋 Listing available models in Ollama...")
    
    try:
        result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
        lines = result.stdout.strip().split('\n')
        
        if len(lines) <= 1:
            if ui:
                ui.print_warning("No models found. You may need to pull a model first.")
            else:
                print("⚠️ No models found. You may need to pull a model first.")
            return []
            
        # Filter by keyword if provided
        if keyword:
            filtered_lines = [line for line in lines if keyword.lower() in line.lower()]
            if ui:
                if filtered_lines:
                    # Extract headers and data
                    headers = [h.strip() for h in lines[0].split()]
                    rows = []
                    
                    for line in filtered_lines[1:] if len(filtered_lines) > 1 else []:
                        if line.strip():
                            rows.append(line.split())
                    
                    # Display as table
                    ui.create_table(headers, rows)
                    ui.print_info(f"Filtered models by: {keyword}")
                else:
                    ui.print_warning(f"No models found matching '{keyword}'")
            else:
                if filtered_lines:
                    print(f"\n🔍 Models matching '{keyword}':")
                    for line in filtered_lines:
                        print(line)
                else:
                    print(f"⚠️ No models found matching '{keyword}'")
        else:
            # Display all models
            if ui:
                # Extract headers and data
                headers = [h.strip() for h in lines[0].split()]
                rows = []
                
                for line in lines[1:]:
                    if line.strip():
                        rows.append(line.split())
                
                # Display as table
                ui.create_table(headers, rows)
            else:
                for line in lines:
                    print(line)
        
        # Show library info
        if ui:
            ui.print_info("Need more models? Check the Ollama library: https://ollama.com/library")
        else:
            print("\n🌐 If you don't see your desired models, you can check available models at: https://ollama.com/library")
        
        # Return models as a list
        models = []
        for line in lines[1:]:  # Skip header
            if line.strip():
                model_name = line.split()[0]
                models.append(model_name)
        return models
    except Exception as e:
        if ui:
            ui.print_error(f"Failed to list models: {e}")
        else:
            print(f"❌ Failed to list models: {e}")
        return []

# Pull DeepSeek model
def pull_deepseek_model(ui=None):
    """Allow user to select and pull a DeepSeek model."""
    # List common DeepSeek model options
    deepseek_options = [
        "deepseek-r1:latest",
        "deepseek-coder:latest",
        "deepseek-coder:6.7b",
        "deepseek-coder:33b",
        "deepseek-llm:latest", 
        "deepseek-llm:7b",
        "deepseek-llm:67b"
    ]
    
    if ui:
        ui.print_section_header("Select DeepSeek Model")
        
        options = deepseek_options + ["Custom model name", "Search available models"]
        choice_idx, choice = ui.prompt_choice("Select a model to pull:", options)
        
        if choice_idx <= len(deepseek_options):
            model_name = choice
        elif choice_idx == len(deepseek_options) + 1:
            model_name = ui.prompt_input("Enter custom DeepSeek model name")
        else:
            keyword = ui.prompt_input("Enter search term for models")
            list_available_models(ui, keyword)
            return pull_deepseek_model(ui)
    else:
        print("\n🔍 Common DeepSeek model options:")
        for i, model in enumerate(deepseek_options, 1):
            print(f"{i}. {model}")
        print(f"{len(deepseek_options) + 1}. Custom model name")
        print(f"{len(deepseek_options) + 2}. Search available models")
        
        # Ask user to select a model
        while True:
            choice = input("\nEnter your choice (number): ").strip()
            try:
                choice_num = int(choice)
                if 1 <= choice_num <= len(deepseek_options):
                    model_name = deepseek_options[choice_num - 1]
                    break
                elif choice_num == len(deepseek_options) + 1:
                    model_name = input("Enter custom DeepSeek model name: ").strip()
                    break
                elif choice_num == len(deepseek_options) + 2:
                    keyword = input("Enter search term for models: ").strip()
                    list_available_models(keyword=keyword)
                    continue
                else:
                    print("Invalid choice. Please try again.")
            except ValueError:
                print("Please enter a valid number.")
    
    if ui:
        ui.print_info(f"Pulling DeepSeek model: {model_name} (this may take some time)...")
    else:
        print(f"\n📥 Pulling DeepSeek model: {model_name} (this may take some time)...")
    
    !ollama pull {model_name}
    
    # Verify model is available
    try:
        response = requests.get("http://localhost:11434/api/tags")
        models = [model['name'] for model in response.json().get('models', [])]
        if model_name in models:
            if ui:
                ui.print_success(f"{model_name} model has been successfully pulled!")
            else:
                print(f"✅ {model_name} model has been successfully pulled!")
            return model_name
        else:
            if ui:
                ui.print_warning(f"Verification failed. Let's check if the model is available:")
                list_available_models(ui, model_name.split(':')[0])
                confirm = ui.prompt_input("Did you see your model in the list above? (yes/no)").lower()
            else:
                print(f"⚠️ Verification failed. Let's check if the model is available:")
                !ollama list | grep -i {model_name.split(':')[0]}
                confirm = input("\nDid you see your model in the list above? (yes/no): ").lower()
                
            if confirm.startswith('y'):
                return model_name
            else:
                if ui:
                    ui.print_error(f"{model_name} model not found after pulling.")
                else:
                    print(f"❌ {model_name} model not found after pulling.")
                return None
    except:
        if ui:
            ui.print_error("Failed to verify model.")
            # Try to list models anyway
            list_available_models(ui)
            confirm = ui.prompt_input("Did you see your model in the list above? (yes/no)").lower()
        else:
            print("❌ Failed to verify model.")
            # Try to list models anyway
            !ollama list
            confirm = input("\nDid you see your model in the list above? (yes/no): ").lower()
            
        if confirm.startswith('y'):
            return model_name
        return None

# Function to interact with DeepSeek model
def query_deepseek(prompt, model, stream=True, system_prompt=None, max_tokens=2048, temperature=0.7, 
                   top_p=0.9, context=None, format=None, num_predict=None):
    """Query a DeepSeek model with extended parameters."""
    url = "http://localhost:11434/api/generate"
    
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": stream,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": top_p,
    }
    
    if system_prompt:
        payload["system"] = system_prompt
    
    if context:
        payload["context"] = context
    
    if format:
        payload["format"] = format
    
    if num_predict:
        payload["num_predict"] = num_predict
    
    if stream:
        response = requests.post(url, json=payload, stream=True)
        response_text = ""
        
        for line in response.iter_lines():
            if line:
                try:
                    chunk = json.loads(line)
                    if 'response' in chunk:
                        response_text += chunk['response']
                        # Print response token by token
                        print(chunk['response'], end='', flush=True)
                    if chunk.get('done', False):
                        break
                except json.JSONDecodeError:
                    print(f"Failed to decode JSON: {line}")
        
        print("\n")  # Add newline after completion
        return response_text
    else:
        start_time = time.time()
        response = requests.post(url, json=payload)
        end_time = time.time()
        
        if response.status_code == 200:
            result = response.json()
            # Add timing information
            result['elapsed_time'] = end_time - start_time
            return result
        else:
            print(f"Error: {response.status_code}, {response.text}")
            return {"error": response.text, "status_code": response.status_code}

# Load text file content
def load_file_content(file_path, ui=None):
    """Load content from a text file to use as context."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        if ui:
            ui.print_success(f"Loaded file: {file_path} ({len(content)} characters)")
        else:
            print(f"✅ Loaded file: {file_path} ({len(content)} characters)")
        return content
    except Exception as e:
        if ui:
            ui.print_error(f"Error loading file {file_path}: {e}")
        else:
            print(f"❌ Error loading file {file_path}: {e}")
        return None

# Function to upload a file
def upload_file(ui=None):
    """Upload a file for processing in Colab."""
    if IN_COLAB:
        from google.colab import files
        try:
            if ui:
                ui.print_info("Please select a file to upload...")
            else:
                print("📤 Please select a file to upload...")
                
            uploaded = files.upload()
            for filename in uploaded.keys():
                # Ensure directory exists
                if not os.path.exists("./input_files"):
                    os.makedirs("./input_files")
                    
                # Save to input_files directory
                with open(f"./input_files/{filename}", "wb") as f:
                    f.write(uploaded[filename])
                
                if ui:
                    ui.print_success(f"Saved uploaded file to ./input_files/{filename}")
                else:
                    print(f"✅ Saved uploaded file to ./input_files/{filename}")
                return f"./input_files/{filename}"
        except Exception as e:
            if ui:
                ui.print_error(f"Error during file upload: {e}")
            else:
                print(f"❌ Error during file upload: {e}")
            return None
    else:
        if ui:
            ui.print_warning("File upload is only available in Google Colab.")
            file_path = ui.prompt_input("Enter path to file")
        else:
            print("⚠️ File upload is only available in Google Colab.")
            file_path = input("Enter path to file: ").strip()
            
        if os.path.exists(file_path):
            return file_path
        else:
            if ui:
                ui.print_error("File not found.")
            else:
                print("❌ File not found.")
            return None

# Save chat history to file
def save_chat_history(chat_history, model_name, ui=None):
    """Save chat history to a JSON file."""
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"./chat_history/chat_{model_name.replace(':', '_')}_{timestamp}.json"
    
    try:
        # Ensure directory exists
        if not os.path.exists("./chat_history"):
            os.makedirs("./chat_history")
            
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                "model": model_name,
                "timestamp": timestamp,
                "history": chat_history
            }, f, indent=2)
            
        if ui:
            ui.print_success(f"Chat history saved to: {filename}")
        else:
            print(f"✅ Chat history saved to: {filename}")
        return filename
    except Exception as e:
        if ui:
            ui.print_error(f"Error saving chat history: {e}")
        else:
            print(f"❌ Error saving chat history: {e}")
        return None

# Load chat history from file
def load_chat_history(ui=None):
    """Load a previously saved chat history."""
    history_files = glob.glob("./chat_history/*.json")
    
    if not history_files:
        if ui:
            ui.print_error("No saved chat history found.")
        else:
            print("❌ No saved chat history found.")
        return None
    
    if ui:
        ui.print_section_header("Available Chat History Files")
        
        # Extract file information
        file_info = []
        for filepath in history_files:
            filename = os.path.basename(filepath)
            timestamp = os.path.getmtime(filepath)
            timestamp_str = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(timestamp))
            size = os.path.getsize(filepath)
            size_str = f"{size / 1024:.1f} KB"
            
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    model = data.get('model', 'Unknown')
                    message_count = len(data.get('history', []))
            except:
                model = "Unknown"
                message_count = "Error"
            
            file_info.append([filename, timestamp_str, model, message_count, size_str])
        
        # Display as table
        headers = ["Filename", "Date", "Model", "Messages", "Size"]
        ui.create_table(headers, file_info)
        
        # Get user choice
        choice_idx, choice = ui.prompt_choice("Select a file to load (or 0 to cancel):", 
                                             [f[0] for f in file_info] + ["Cancel"])
        
        if choice_idx == len(file_info) + 1:  # Cancel option
            return None
        
        try:
            filepath = history_files[choice_idx - 1]
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            ui.print_success(f"Loaded chat history with model: {data.get('model', 'Unknown')}")
            return data
        except Exception as e:
            ui.print_error(f"Error loading chat history: {e}")
            return None
    else:
        print("\n📋 Available chat history files:")
        for i, filepath in enumerate(history_files, 1):
            filename = os.path.basename(filepath)
            print(f"{i}. {filename}")
        
        while True:
            choice = input("\nEnter your choice (number) or 0 to cancel: ").strip()
            try:
                choice_num = int(choice)
                if choice_num == 0:
                    return None
                if 1 <= choice_num <= len(history_files):
                    filepath = history_files[choice_num - 1]
                    
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    print(f"✅ Loaded chat history with model: {data['model']}")
                    return data
                else:
                    print("Invalid choice. Please try again.")
            except ValueError:
                print("Please enter a valid number.")
            except Exception as e:
                print(f"❌ Error loading chat history: {e}")
                return None

# Monitor system resources
class ResourceMonitor:
    """Monitor CPU, RAM, and GPU (if available) usage."""
    def __init__(self, interval=1.0, ui=None):
        self.interval = interval
        self.running = False
        self.data = {
            'time': [],
            'cpu': [],
            'ram': [],
            'gpu': []
        }
        self.ui = ui
    
    def start(self):
        """Start monitoring resources."""
        self.running = True
        self.data = {'time': [], 'cpu': [], 'ram': [], 'gpu': []}
        self.thread = threading.Thread(target=self._monitor)
        self.thread.daemon = True
        self.thread.start()
        if self.ui:
            self.ui.print_info("Resource monitoring started")
        else:
            print("📊 Resource monitoring started")
    
    def stop(self):
        """Stop monitoring resources."""
        self.running = False
        if hasattr(self, 'thread'):
            self.thread.join(timeout=2.0)
        if self.ui:
            self.ui.print_info("Resource monitoring stopped")
        else:
            print("📊 Resource monitoring stopped")
        return self.data
    
    def _monitor(self):
        """Internal monitoring function."""
        start_time = time.time()
        while self.running:
            current_time = time.time() - start_time
            
            # CPU and RAM
            cpu_percent = psutil.cpu_percent()
            ram_percent = psutil.virtual_memory().percent
            
            # GPU if available
            gpu_percent = None
            if TORCH_AVAILABLE and torch.cuda.is_available():
                try:
                    gpu_percent = torch.cuda.utilization()
                except:
                    try:
                        # Alternative method
                        result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                                                capture_output=True, text=True)
                        gpu_percent = float(result.stdout.strip())
                    except:
                        gpu_percent = None
            
            # Save data
            self.data['time'].append(current_time)
            self.data['cpu'].append(cpu_percent)
            self.data['ram'].append(ram_percent)
            self.data['gpu'].append(gpu_percent)
            
            time.sleep(self.interval)
    
    def plot(self):
        """Plot the collected resource usage data."""
        if not self.data['time']:
            if self.ui:
                self.ui.print_error("No monitoring data available to plot")
            else:
                print("❌ No monitoring data available to plot")
            return
        
        fig, ax1 = plt.subplots(figsize=(10, 6))
        
        # CPU and RAM
        ax1.set_xlabel('Time (seconds)')
        ax1.set_ylabel('Usage (%)')
        ax1.plot(self.data['time'], self.data['cpu'], 'b-', label='CPU')
        ax1.plot(self.data['time'], self.data['ram'], 'g-', label='RAM')
        
        # GPU if available
        if any(x is not None for x in self.data['gpu']):
            ax1.plot(self.data['time'], self.data['gpu'], 'r-', label='GPU')
        
        ax1.set_ylim(0, 100)
        ax1.legend(loc='upper left')
        ax1.grid(True)
        
        plt.title('System Resource Usage')
        plt.tight_layout()
        plt.show()

# Interactive chat with DeepSeek model
def interactive_chat(model_name, ui=None, system_prompt=None, initial_prompt=None, 
                     chat_history=None, max_tokens=2048, temperature=0.7, 
                     monitor_resources=False):
    """Interactive chat with a model with enhanced UI."""
    if ui:
        ui.print_section_header(f"Chat with {model_name}")
        
        # Display system prompt if provided
        if system_prompt:
            ui.display_chat_message("system", f"Using system prompt: \"{system_prompt}\"")
        
        # Available commands
        commands = {
            "/save": "Save the chat history",
            "/file": "Upload and use a file as context",
            "/reset": "Reset the conversation",
            "/system": "Change the system prompt (usage: /system Your new prompt)",
            "/help": "Show available commands",
            "/theme": "Change the UI theme",
            "/exit": "End the conversation",
            "/quit": "End the conversation"
        }
        
        ui.print_info("Type /help to see available commands or /exit to end the conversation")
    else:
        print(f"\n🤖 Starting interactive chat with {model_name}...")
        print("🔍 Type 'exit', 'quit', or press Ctrl+C to end the conversation")
        print("📝 Special commands:")
        print("  - /save : Save the chat history")
        print("  - /file : Upload and use a file as context")
        print("  - /reset : Reset the conversation")
        print("  - /system <prompt> : Change the system prompt")
        print("  - /help : Show available commands")
        
        if system_prompt:
            print(f"\n🔧 Using system prompt: \"{system_prompt}\"")
    
    # Initialize or use provided chat history
    if chat_history is None:
        chat_history = []
    
    # Resource monitoring
    monitor = None
    if monitor_resources:
        monitor = ResourceMonitor(ui=ui)
        monitor.start()
    
    # Context for file input
    file_context = None
    
    try:
        # If an initial prompt was provided, use it first
        if initial_prompt:
            if ui:
                ui.display_chat_message("user", initial_prompt)
            else:
                print(f"\n🧑 Initial Prompt: {initial_prompt}")
                
            chat_history.append({"role": "user", "content": initial_prompt})
            
            if ui:
                ui.display_thinking(model_name=model_name)
                
                # Query without streaming for UI version
                response = query_deepseek(
                    initial_prompt, 
                    model=model_name,
                    system_prompt=system_prompt,
                    stream=False,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    context=file_context
                )
                
                # Display with typing effect
                if 'error' not in response:
                    ui.live_response(response.get('response', ''), model_name=model_name)
                    chat_history.append({"role": "assistant", "content": response.get('response', '')})
                else:
                    ui.print_error(f"Error: {response.get('error', 'Unknown error')}")
            else:
                print(f"\n🤖 {model_name}: ", end="")
                
                # Regular streaming response
                response = query_deepseek(
                    initial_prompt, 
                    model=model_name,
                    system_prompt=system_prompt,
                    stream=True,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    context=file_context
                )
                
                chat_history.append({"role": "assistant", "content": response})
        
        # Continue with interactive chat
        while True:
            if ui:
                user_input = ui.prompt_input("You")
            else:
                user_input = input("\n🧑 You: ")
            
            # Check for special commands
            if user_input.lower() in ['exit', 'quit', '/exit', '/quit']:
                if ui:
                    ui.print_info("Ending conversation.")
                else:
                    print("\n👋 Ending conversation.")
                break
            
            elif user_input.lower() == '/save':
                if ui:
                    ui.save_chat_history()
                else:
                    save_path = save_chat_history(chat_history, model_name)
                continue
            
            elif user_input.lower() == '/file':
                if ui:
                    file_path = upload_file(ui)
                else:
                    file_path = upload_file()
                    
                if file_path:
                    if ui:
                        file_context = load_file_content(file_path, ui)
                        ui.print_success(f"File content loaded as context. ({len(file_context)} characters)")
                    else:
                        file_context = load_file_content(file_path)
                        print(f"✅ File content loaded as context. ({len(file_context)} characters)")
                continue
            
            elif user_input.lower() == '/reset':
                chat_history = []
                file_context = None
                if ui:
                    ui.print_success("Conversation reset.")
                else:
                    print("🔄 Conversation reset.")
                continue
            
            elif user_input.lower().startswith('/system '):
                system_prompt = user_input[8:].strip()
                if ui:
                    ui.print_success(f"System prompt updated: \"{system_prompt}\"")
                else:
                    print(f"🔧 System prompt updated: \"{system_prompt}\"")
                continue
            
            elif user_input.lower() == '/theme' and ui:
                themes = ["Blue", "Green", "Purple", "Dark"]
                theme_idx, theme_name = ui.prompt_choice("Choose a new theme:", themes)
                ui.set_theme(theme_name.lower())
                ui.print_success(f"Theme changed to {theme_name}")
                continue
            
            elif user_input.lower() == '/help':
                if ui:
                    ui.display_help(commands)
                else:
                    print("\n📝 Available commands:")
                    print("  - /save : Save the chat history")
                    print("  - /file : Upload and use a file as context")
                    print("  - /reset : Reset the conversation")
                    print("  - /system <prompt> : Change the system prompt")
                    print("  - /help : Show this help message")
                    print("  - exit, quit : End the conversation")
                continue
            
            # Regular chat message
            if ui:
                ui.display_chat_message("user", user_input)
            
            chat_history.append({"role": "user", "content": user_input})
            
            if ui:
                ui.display_thinking(model_name=model_name)
                
                # Query without streaming for UI version
                response = query_deepseek(
                    user_input, 
                    model=model_name,
                    system_prompt=system_prompt,
                    stream=False,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    context=file_context
                )
                
                # Display with typing effect
                if 'error' not in response:
                    ui.live_response(response.get('response', ''), model_name=model_name)
                    chat_history.append({"role": "assistant", "content": response.get('response', '')})
                else:
                    ui.print_error(f"Error: {response.get('error', 'Unknown error')}")
            else:
                print(f"\n🤖 {model_name}: ", end="")
                
                # Regular streaming response
                response = query_deepseek(
                    user_input, 
                    model=model_name,
                    system_prompt=system_prompt,
                    stream=True,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    context=file_context
                )
                
                chat_history.append({"role": "assistant", "content": response})
    
    except KeyboardInterrupt:
        if ui:
            ui.print_warning("Conversation interrupted.")
        else:
            print("\n\n👋 Conversation interrupted.")
    
    finally:
        # Stop resource monitoring if active
        if monitor_resources and monitor:
            data = monitor.stop()
            if ui:
                save_monitoring = ui.prompt_input("Do you want to see the resource usage graph? (yes/no)")
            else:
                save_monitoring = input("\nDo you want to see the resource usage graph? (yes/no): ").lower()
                
            if save_monitoring.lower().startswith('y'):
                monitor.plot()
        
        # Offer to save chat history
        if ui:
            save_option = ui.prompt_input("Do you want to save this chat history? (yes/no)")
        else:
            save_option = input("\nDo you want to save this chat history? (yes/no): ").lower()
            
        if save_option.lower().startswith('y'):
            if ui:
                save_path = ui.save_chat_history()
            else:
                save_path = save_chat_history(chat_history, model_name)
        
        if ui:
            ui.print_info("Chat session ended.")
        else:
            print("\n👋 Chat session ended.")

# Benchmark models or parameters
def benchmark_models(ui=None):
    """Benchmark different models or parameter settings."""
    if ui:
        ui.print_section_header("Model Benchmarking Tool")
    else:
        print("\n📊 Model Benchmarking Tool")
    
    # Get available models
    available_models = list_available_models(ui)
    
    if not available_models:
        if ui:
            ui.print_error("No models available for benchmarking.")
        else:
            print("❌ No models available for benchmarking.")
        return
    
    # Select models to benchmark
    if ui:
        choice_idx, choice = ui.prompt_choice(
            "Select models to benchmark:", 
            available_models + ["All models"]
        )
        
        if choice_idx == len(available_models) + 1:  # All models
            benchmark_models = available_models
        else:
            benchmark_models = [choice]
    else:
        print("\nSelect models to benchmark:")
        for i, model in enumerate(available_models, 1):
            print(f"{i}. {model}")
        
        model_choices = input("\nEnter model numbers to benchmark (comma-separated) or 'all': ").strip()
        
        benchmark_models = []
        if model_choices.lower() == 'all':
            benchmark_models = available_models
        else:
            try:
                indices = [int(idx.strip()) - 1 for idx in model_choices.split(',')]
                benchmark_models = [available_models[idx] for idx in indices if 0 <= idx < len(available_models)]
            except:
                print("❌ Invalid selection. Using first model only.")
                benchmark_models = [available_models[0]]
    
    if ui:
        ui.print_success(f"Models to benchmark: {', '.join(benchmark_models)}")
    else:
        print(f"\n✅ Models to benchmark: {', '.join(benchmark_models)}")
    
    # Get benchmark parameters
    if ui:
        ui.print_section_header("Benchmark Parameters")
        
        # Get prompts
        prompt_source_options = ["Input prompts manually", "Load from file"]
        source_idx, source_choice = ui.prompt_choice("Enter prompt source:", prompt_source_options)
        
        prompts = []
        if source_idx == 2:  # Load from file
            file_path = upload_file(ui)
            if file_path:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        prompts = [line.strip() for line in f if line.strip()]
                    ui.print_success(f"Loaded {len(prompts)} prompts from file")
                except Exception as e:
                    ui.print_error(f"Error loading prompts: {e}")
                    return
        else:  # Manual input
            num_prompts = ui.prompt_input("Enter number of prompts to benchmark")
            try:
                num_prompts = int(num_prompts)
            except:
                num_prompts = 1
                ui.print_warning("Invalid number, using 1 prompt")
                
            for i in range(num_prompts):
                prompt = ui.prompt_input(f"Enter prompt {i+1}/{num_prompts}")
                prompts.append(prompt)
                
        # Get other parameters
        system_prompt = ui.prompt_input("Enter system prompt (or leave empty)")
        
        max_tokens = ui.prompt_input("Enter max tokens (default: 1024)")
        try:
            max_tokens = int(max_tokens) if max_tokens else 1024
        except:
            max_tokens = 1024
            ui.print_warning("Invalid number, using default: 1024")
            
        temperature = ui.prompt_input("Enter temperature (default: 0.7)")
        try:
            temperature = float(temperature) if temperature else 0.7
        except:
            temperature = 0.7
            ui.print_warning("Invalid number, using default: 0.7")
            
        runs = ui.prompt_input("Enter number of runs per prompt (default: 1)")
        try:
            runs = int(runs) if runs else 1
        except:
            runs = 1
            ui.print_warning("Invalid number, using default: 1")
    else:
        print("\n📝 Enter benchmark parameters:")
        prompts = []
        
        # Get prompts
        prompt_source = input("Enter prompt source (1. Input prompts, 2. Load from file): ").strip()
        
        if prompt_source == '2':
            file_path = upload_file()
            if file_path:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        prompts = [line.strip() for line in f if line.strip()]
                    print(f"✅ Loaded {len(prompts)} prompts from file")
                except Exception as e:
                    print(f"❌ Error loading prompts: {e}")
                    return
        else:
            num_prompts = input("Enter number of prompts to benchmark: ").strip()
            try:
                num_prompts = int(num_prompts)
            except:
                num_prompts = 1
            
            for i in range(num_prompts):
                prompt = input(f"Enter prompt {i+1}/{num_prompts}: ").strip()
                prompts.append(prompt)
        
        # Get system prompt
        system_prompt = input("Enter system prompt (or leave empty): ").strip()
        
        # Get max tokens
        max_tokens = input("Enter max tokens (default: 1024): ").strip()
        try:
            max_tokens = int(max_tokens)
        except:
            max_tokens = 1024
        
        # Get temperature
        temperature = input("Enter temperature (default: 0.7): ").strip()
        try:
            temperature = float(temperature)
        except:
            temperature = 0.7
        
        # Number of runs per model
        runs = input("Enter number of runs per prompt (default: 1): ").strip()
        try:
            runs = int(runs)
        except:
            runs = 1
    
    if not prompts:
        if ui:
            ui.print_error("No prompts provided for benchmarking.")
        else:
            print("❌ No prompts provided for benchmarking.")
        return
    
    # Prepare results
    results = []
    
    # Run benchmarks
    if ui:
        ui.print_section_header(f"Running Benchmarks")
        ui.print_info(f"Testing {len(benchmark_models)} models × {len(prompts)} prompts × {runs} runs...")
    else:
        print(f"\n🚀 Running benchmarks ({len(benchmark_models)} models × {len(prompts)} prompts × {runs} runs)...")
    
    progress_total = len(benchmark_models) * len(prompts) * runs
    progress_bar = tqdm(total=progress_total, desc="Benchmarking")
    
    for model_name in benchmark_models:
        for prompt in prompts:
            for run in range(runs):
                # Query model with stream=False to get timing info
                result = query_deepseek(
                    prompt=prompt[:50] + "..." if len(prompt) > 50 else prompt,
                    model=model_name,
                    stream=False,
                    system_prompt=system_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
                if 'error' not in result:
                    # Extract results
                    results.append({
                        'model': model_name,
                        'prompt': prompt[:50] + "..." if len(prompt) > 50 else prompt,
                        'run': run + 1,
                        'time': result.get('elapsed_time', 0),
                        'tokens': len(result.get('response', '').split())
                    })
                
                progress_bar.update(1)
                # Small delay to avoid overwhelming the server
                time.sleep(0.5)
    
    progress_bar.close()
    
    # Show results
    if results:
        # Convert to DataFrame for analysis
        df = pd.DataFrame(results)
        
        # Calculate average time and tokens per model
        model_stats = df.groupby('model').agg({
            'time': ['mean', 'std'],
            'tokens': ['mean', 'std']
        })
        
        if ui:
            ui.print_section_header("Benchmark Results")
            
            # Create simplified table for display
            stats_table = []
            for model in model_stats.index:
                mean_time = model_stats.loc[model, ('time', 'mean')]
                std_time = model_stats.loc[model, ('time', 'std')]
                mean_tokens = model_stats.loc[model, ('tokens', 'mean')]
                std_tokens = model_stats.loc[model, ('tokens', 'std')]
                
                stats_table.append([
                    model,
                    f"{mean_time:.2f}s ± {std_time:.2f}",
                    f"{mean_tokens:.1f} ± {std_tokens:.1f}"
                ])
            
            ui.create_table(["Model", "Time (mean±std)", "Tokens (mean±std)"], stats_table)
        else:
            print("\n📊 Benchmark Results:")
            print(model_stats)
        
        # Plot results
        plt.figure(figsize=(12, 6))
        
        plt.subplot(1, 2, 1)
        df.groupby('model')['time'].mean().plot(kind='bar')
        plt.title('Average Response Time by Model')
        plt.ylabel('Time (seconds)')
        plt.xticks(rotation=45)
        
        plt.subplot(1, 2, 2)
        df.groupby('model')['tokens'].mean().plot(kind='bar')
        plt.title('Average Tokens by Model')
        plt.ylabel('Tokens')
        plt.xticks(rotation=45)
        
        plt.tight_layout()
        plt.show()
        
        # Save results
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        # Ensure directory exists
        if not os.path.exists("./benchmarks"):
            os.makedirs("./benchmarks")
            
        result_file = f"./benchmarks/benchmark_{timestamp}.csv"
        df.to_csv(result_file, index=False)
        
        if ui:
            ui.print_success(f"Benchmark results saved to {result_file}")
        else:
            print(f"\n✅ Benchmark results saved to {result_file}")
    else:
        if ui:
            ui.print_error("No valid benchmark results collected.")
        else:
            print("❌ No valid benchmark results collected.")

# Multi-model comparison
def compare_models(ui=None):
    """Compare multiple models with the same prompt."""
    if ui:
        ui.print_section_header("Model Comparison Tool")
    else:
        print("\n🔍 Model Comparison Tool")
    
    # Get available models
    available_models = list_available_models(ui)
    
    if not available_models:
        if ui:
            ui.print_error("No models available for comparison.")
        else:
            print("❌ No models available for comparison.")
        return
    
    # Select models to compare
    if ui:
        ui.print_info("Select up to 4 models to compare side by side")
        
        compare_models_list = []
        while len(compare_models_list) < 4:
            # Filter out already selected models
            remaining_models = [m for m in available_models if m not in compare_models_list]
            
            if not remaining_models:
                break
                
            options = remaining_models + ["Finish selection"]
            choice_idx, choice = ui.prompt_choice(
                f"Select model {len(compare_models_list)+1} (or finish):", 
                options
            )
            
            if choice_idx == len(remaining_models) + 1:  # Finish
                break
            else:
                compare_models_list.append(choice)
                ui.print_success(f"Added {choice} to comparison")
    else:
        print("\nSelect models to compare:")
        for i, model in enumerate(available_models, 1):
            print(f"{i}. {model}")
        
        model_choices = input("\nEnter model numbers to compare (comma-separated, max 4): ").strip()
        
        compare_models_list = []
        try:
            indices = [int(idx.strip()) - 1 for idx in model_choices.split(',')]
            compare_models_list = [available_models[idx] for idx in indices if 0 <= idx < len(available_models)]
            # Limit to 4 models for comparison
            compare_models_list = compare_models_list[:4]
        except:
            print("❌ Invalid selection. Using first model only.")
            compare_models_list = [available_models[0]]
    
    if len(compare_models_list) < 2:
        if ui:
            ui.print_warning("Need at least 2 models for comparison. Adding another model.")
        else:
            print("⚠️ Need at least 2 models for comparison. Adding another model.")
            
        if len(available_models) > 1:
            for model in available_models:
                if model not in compare_models_list:
                    compare_models_list.append(model)
                    break
    
    if ui:
        ui.print_success(f"Models to compare: {', '.join(compare_models_list)}")
    else:
        print(f"\n✅ Models to compare: {', '.join(compare_models_list)}")
    
    # Get the prompt to use for comparison
    if ui:
        prompt = ui.prompt_input("Enter prompt for comparison")
        if not prompt:
            prompt = "Explain the advantages and applications of large language models in three paragraphs."
            ui.print_info(f"Using default prompt: \"{prompt}\"")
        
        # Get system prompt
        system_prompt = ui.prompt_input("Enter system prompt (or leave empty)")
        
        # Get max tokens
        max_tokens = ui.prompt_input("Enter max tokens (default: 1024)")
        try:
            max_tokens = int(max_tokens) if max_tokens else 1024
        except:
            max_tokens = 1024
            ui.print_warning("Invalid value, using default: 1024")
        
        # Get temperature
        temperature = ui.prompt_input("Enter temperature (default: 0.7)")
        try:
            temperature = float(temperature) if temperature else 0.7
        except:
            temperature = 0.7
            ui.print_warning("Invalid value, using default: 0.7")
    else:
        # Get the prompt to use for comparison
        prompt = input("\nEnter prompt for comparison: ").strip()
        if not prompt:
            prompt = "Explain the advantages and applications of large language models in three paragraphs."
            print(f"Using default prompt: \"{prompt}\"")
        
        # Get system prompt
        system_prompt = input("Enter system prompt (or leave empty): ").strip()
        
        # Get max tokens
        max_tokens = input("Enter max tokens (default: 1024): ").strip()
        try:
            max_tokens = int(max_tokens)
        except:
            max_tokens = 1024
        
        # Get temperature
        temperature = input("Enter temperature (default: 0.7): ").strip()
        try:
            temperature = float(temperature)
        except:
            temperature = 0.7
    
    # Run comparison
    if ui:
        ui.print_section_header("Running Model Comparison")
    else:
        print("\n🔄 Running model comparison...")
    
    results = {}
    
    for model_name in compare_models_list:
        if ui:
            ui.print_info(f"Generating response from {model_name}...")
        else:
            print(f"\n🤖 Model: {model_name}")
            print("Generating response...", end="", flush=True)
        
        start_time = time.time()
        response = query_deepseek(
            prompt=prompt,
            model=model_name,
            stream=False,
            system_prompt=system_prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )
        elapsed_time = time.time() - start_time
        
        if ui:
            ui.print_success(f"Response from {model_name} generated in {elapsed_time:.2f} seconds")
        else:
            print(" Done!")
        
        if 'error' not in response:
            # Store results
            results[model_name] = {
                'response': response.get('response', ''),
                'time': elapsed_time,
                'tokens': len(response.get('response', '').split())
            }
    
    # Display results
    if results:
        if ui:
            ui.print_section_header("Comparison Results")
            
            # Show time and token statistics
            stats_table = []
            for model, data in results.items():
                stats_table.append([
                    model, 
                    f"{data['time']:.2f} seconds",
                    f"{data['tokens']} tokens"
                ])
            
            ui.create_table(["Model", "Response Time", "Tokens"], stats_table)
            
            # Show responses
            for model, data in results.items():
                ui.print_section_header(f"Response from {model}")
                ui.display_chat_message("assistant", data['response'], model_name=model)
        else:
            print("\n📊 Comparison Results:")
            
            # Show time and token statistics
            print("\n⏱️ Response Times:")
            for model, data in results.items():
                print(f"  - {model}: {data['time']:.2f} seconds ({data['tokens']} tokens)")
            
            # Show responses side-by-side in a grid
            print("\n🔍 Model Responses:")
            for i, model in enumerate(results.keys(), 1):
                print(f"\n📝 Model {i}: {model}")
                print("-" * 80)
                print(results[model]['response'])
                print("-" * 80)
        
        # Save comparison
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        # Ensure directory exists
        if not os.path.exists("./benchmarks"):
            os.makedirs("./benchmarks")
            
        comparison_file = f"./benchmarks/comparison_{timestamp}.json"
        
        with open(comparison_file, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': timestamp,
                'prompt': prompt,
                'system_prompt': system_prompt,
                'max_tokens': max_tokens,
                'temperature': temperature,
                'results': results
            }, f, indent=2)
        
        if ui:
            ui.print_success(f"Comparison results saved to {comparison_file}")
        else:
            print(f"\n✅ Comparison results saved to {comparison_file}")
    else:
        if ui:
            ui.print_error("No valid comparison results collected.")
        else:
            print("❌ No valid comparison results collected.")

# Batch processing of prompts
def batch_processing(ui=None):
    """Process multiple prompts in batch mode."""
    if ui:
        ui.print_section_header("Batch Processing Tool")
    else:
        print("\n📚 Batch Processing Tool")
    
    # Select a model
    available_models = list_available_models(ui)
    
    if not available_models:
        if ui:
            ui.print_error("No models available for batch processing.")
        else:
            print("❌ No models available for batch processing.")
        return
    
    # Select model
    if ui:
        choice_idx, model_name = ui.prompt_choice("Select a model for batch processing:", available_models)
    else:
        print("\nSelect a model for batch processing:")
        for i, model in enumerate(available_models, 1):
            print(f"{i}. {model}")
        
        model_choice = input("\nEnter model number: ").strip()
        try:
            model_idx = int(model_choice) - 1
            if 0 <= model_idx < len(available_models):
                model_name = available_models[model_idx]
            else:
                print("❌ Invalid selection. Using first model.")
                model_name = available_models[0]
        except:
            print("❌ Invalid selection. Using first model.")
            model_name = available_models[0]
    
    if ui:
        ui.print_success(f"Using model: {model_name}")
    else:
        print(f"\n✅ Using model: {model_name}")
    
    # Get prompts
    if ui:
        options = [
            "Enter prompts manually",
            "Upload a CSV file with prompts",
            "Upload a text file with one prompt per line"
        ]
        choice_idx, _ = ui.prompt_choice("Select prompt source:", options)
        source_choice = str(choice_idx)
    else:
        print("\nSelect prompt source:")
        print("1. Enter prompts manually")
        print("2. Upload a CSV file with prompts")
        print("3. Upload a text file with one prompt per line")
        
        source_choice = input("\nEnter your choice (1-3): ").strip()
    
    prompts = []
    
    if source_choice == '1':
        if ui:
            num_prompts = ui.prompt_input("Enter number of prompts")
            try:
                num_prompts = int(num_prompts)
            except:
                num_prompts = 1
                ui.print_warning("Invalid number, using 1 prompt")
            
            for i in range(num_prompts):
                prompt = ui.prompt_input(f"Enter prompt {i+1}/{num_prompts}")
                prompts.append(prompt)
        else:
            num_prompts = input("Enter number of prompts: ").strip()
            try:
                num_prompts = int(num_prompts)
            except:
                num_prompts = 1
            
            for i in range(num_prompts):
                prompt = input(f"Enter prompt {i+1}/{num_prompts}: ").strip()
                prompts.append(prompt)
    
    elif source_choice == '2':
        if ui:
            file_path = upload_file(ui)
        else:
            file_path = upload_file()
            
        if file_path:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    csv_reader = csv.reader(f)
                    header = next(csv_reader, None)
                    
                    if header:
                        if ui:
                            ui.print_section_header("CSV Columns")
                            col_options = header + ["Cancel"]
                            choice_idx, _ = ui.prompt_choice("Which column contains the prompts?", col_options)
                            
                            if choice_idx <= len(header):
                                col_idx = choice_idx - 1
                            else:
                                return
                        else:
                            print("\nCSV columns found:")
                            for i, col in enumerate(header):
                                print(f"{i+1}. {col}")
                            
                            col_choice = input("Which column contains the prompts? (number): ").strip()
                            try:
                                col_idx = int(col_choice) - 1
                            except:
                                col_idx = 0
                        
                        # Read prompts from selected column
                        for row in csv_reader:
                            if 0 <= col_idx < len(row):
                                prompts.append(row[col_idx])
                    else:
                        # No header, use first column
                        f.seek(0)
                        for row in csv_reader:
                            if row:
                                prompts.append(row[0])
                
                if ui:
                    ui.print_success(f"Loaded {len(prompts)} prompts from CSV")
                else:
                    print(f"✅ Loaded {len(prompts)} prompts from CSV")
            except Exception as e:
                if ui:
                    ui.print_error(f"Error loading CSV: {e}")
                else:
                    print(f"❌ Error loading CSV: {e}")
                return
    
    elif source_choice == '3':
        if ui:
            file_path = upload_file(ui)
        else:
            file_path = upload_file()
            
        if file_path:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    prompts = [line.strip() for line in f if line.strip()]
                
                if ui:
                    ui.print_success(f"Loaded {len(prompts)} prompts from text file")
                else:
                    print(f"✅ Loaded {len(prompts)} prompts from text file")
            except Exception as e:
                if ui:
                    ui.print_error(f"Error loading text file: {e}")
                else:
                    print(f"❌ Error loading text file: {e}")
                return
    
    if not prompts:
        if ui:
            ui.print_error("No prompts provided for batch processing.")
        else:
            print("❌ No prompts provided for batch processing.")
        return
    
    # Get parameters
    if ui:
        # Get system prompt
        system_prompt = ui.prompt_input("Enter system prompt (or leave empty)")
        
        # Get max tokens
        max_tokens = ui.prompt_input("Enter max tokens (default: 1024)")
        try:
            max_tokens = int(max_tokens) if max_tokens else 1024
        except:
            max_tokens = 1024
            ui.print_warning("Invalid value, using default: 1024")
        
        # Get temperature
        temperature = ui.prompt_input("Enter temperature (default: 0.7)")
        try:
            temperature = float(temperature) if temperature else 0.7
        except:
            temperature = 0.7
            ui.print_warning("Invalid value, using default: 0.7")
    else:
        # Get system prompt
        system_prompt = input("Enter system prompt (or leave empty): ").strip()
        
        # Get max tokens
        max_tokens = input("Enter max tokens (default: 1024): ").strip()
        try:
            max_tokens = int(max_tokens)
        except:
            max_tokens = 1024
        
        # Get temperature
        temperature = input("Enter temperature (default: 0.7): ").strip()
        try:
            temperature = float(temperature)
        except:
            temperature = 0.7
    
    # Process prompts
    if ui:
        ui.print_section_header("Processing Prompts")
        ui.print_info(f"Processing {len(prompts)} prompts in batch mode...")
    else:
        print(f"\n🔄 Processing {len(prompts)} prompts in batch mode...")
    
    results = []
    progress_bar = tqdm(total=len(prompts), desc="Processing")
    
    for i, prompt in enumerate(prompts):
        try:
            if ui:
                ui.display_progress(
                    percentage=int((i/len(prompts))*100), 
                    prefix=f"Processing {i+1}/{len(prompts)}", 
                    suffix=f"{model_name}"
                )
                
            response = query_deepseek(
                prompt=prompt,
                model=model_name,
                stream=False,
                system_prompt=system_prompt,
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            if 'error' not in response:
                results.append({
                    'prompt_id': i + 1,
                    'prompt': prompt,
                    'response': response.get('response', ''),
                    'time': response.get('elapsed_time', 0)
                })
            else:
                results.append({
                    'prompt_id': i + 1,
                    'prompt': prompt,
                    'response': f"ERROR: {response.get('error', 'Unknown error')}",
                    'time': 0
                })
        except Exception as e:
            results.append({
                'prompt_id': i + 1,
                'prompt': prompt,
                'response': f"ERROR: {str(e)}",
                'time': 0
            })
        
        progress_bar.update(1)
        # Small delay to avoid overwhelming the server
        time.sleep(0.5)
    
    progress_bar.close()
    
    # Save results
    if results:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        # Ensure directory exists
        if not os.path.exists("./benchmarks"):
            os.makedirs("./benchmarks")
            
        output_file = f"./benchmarks/batch_{timestamp}.csv"
        
        with open(output_file, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['prompt_id', 'prompt', 'response', 'time'])
            
            for result in results:
                writer.writerow([
                    result['prompt_id'],
                    result['prompt'],
                    result['response'],
                    result['time']
                ])
        
        if ui:
            ui.print_success(f"Batch processing results saved to {output_file}")
        else:
            print(f"\n✅ Batch processing results saved to {output_file}")
        
        # Display summary
        total_time = sum(result['time'] for result in results)
        avg_time = total_time / len(results) if results else 0
        successful = sum(1 for r in results if not str(r['response']).startswith('ERROR'))
        failed = sum(1 for r in results if str(r['response']).startswith('ERROR'))
        
        if ui:
            ui.print_section_header("Batch Processing Summary")
            
            summary_table = [
                ["Total prompts", len(prompts)],
                ["Successful responses", successful],
                ["Failed responses", failed],
                ["Total processing time", f"{total_time:.2f} seconds"],
                ["Average time per prompt", f"{avg_time:.2f} seconds"]
            ]
            
            for row in summary_table:
                print(f"{ui.theme['secondary']}{row[0]}: {ui.theme['reset']}{row[1]}")
        else:
            print(f"\n📊 Batch Processing Summary:")
            print(f"  - Total prompts: {len(prompts)}")
            print(f"  - Successful responses: {successful}")
            print(f"  - Failed responses: {failed}")
            print(f"  - Total processing time: {total_time:.2f} seconds")
            print(f"  - Average time per prompt: {avg_time:.2f} seconds")
    else:
        if ui:
            ui.print_error("No results collected during batch processing.")
        else:
            print("❌ No results collected during batch processing.")

# Web UI with Gradio
def create_web_ui(model_name, ui=None):
    """Create a simple web UI using Gradio."""
    if not GRADIO_AVAILABLE:
        if ui:
            ui.print_error("Gradio is not available. Cannot create web UI.")
            ui.print_info("Try running: !pip install gradio")
        else:
            print("❌ Gradio is not available. Cannot create web UI.")
            print("📝 Try running: !pip install gradio")
        return
    
    if ui:
        ui.print_section_header(f"Creating Web UI for {model_name}")
    else:
        print(f"\n🌐 Creating web UI for model: {model_name}")
    
    # Define system prompt
    system_prompt_input = gr.Textbox(
        label="System Prompt",
        placeholder="You are a helpful AI assistant...",
        lines=2
    )
    
    # Define chat interface
    def respond(message, history, system_prompt):
        # Format prompt from history
        if not history:
            prompt = message
        else:
            prompt = message
        
        # Get response
        response = query_deepseek(
            prompt=message,
            model=model_name,
            stream=False,
            system_prompt=system_prompt
        )
        
        if 'error' in response:
            return f"Error: {response.get('error', 'Unknown error')}"
        
        return response.get('response', '')
    
    # Create interface
    chat_interface = gr.ChatInterface(
        fn=lambda message, history, system_prompt: respond(message, history, system_prompt),
        additional_inputs=[system_prompt_input],
        title=f"DeepSeek {model_name} Chat",
        description=f"Chat with the DeepSeek {model_name} model using Ollama",
        examples=[
            ["Tell me about large language models"],
            ["Write a Python function to calculate Fibonacci numbers"],
            ["Explain quantum computing in simple terms"],
        ],
        theme="default"
    )
    
    # Launch interface
    share_url = chat_interface.launch(share=True, inbrowser=True)
    
    if ui:
        ui.show_web_interface(share_url)
    else:
        print(f"\n✅ Web interface is running!")
        print(f"URL: {share_url}")
        print("\nNote: Keep this tab open while using the web interface.")
        print("Press Ctrl+C to stop the server when you're done.")

# Function to set advanced parameters
def advanced_settings(ui=None):
    """Configure advanced model settings."""
    if ui:
        ui.print_section_header("Advanced Settings")
        
        system_prompt = ui.prompt_input("Enter system prompt (leave empty for default)")
        max_tokens = ui.prompt_input("Enter max tokens (default: 2048)")
        temperature = ui.prompt_input("Enter temperature (0-1, default: 0.7)")
        top_p = ui.prompt_input("Enter top_p (0-1, default: 0.9)")
        
        monitor_options = ["Yes", "No"]
        monitor_idx, _ = ui.prompt_choice("Monitor system resources?", monitor_options)
        monitor_resources = (monitor_idx == 1)
    else:
        print("\n⚙️ Advanced Settings:")
        system_prompt = input("Enter system prompt (leave empty for default): ").strip()
        max_tokens = input("Enter max tokens (default: 2048): ").strip()
        temperature = input("Enter temperature (0-1, default: 0.7): ").strip()
        top_p = input("Enter top_p (0-1, default: 0.9): ").strip()
        monitor_resources = input("Monitor system resources? (yes/no, default: no): ").strip().lower()
    
    settings = {}
    
    if system_prompt:
        settings["system_prompt"] = system_prompt
    
    if max_tokens:
        try:
            settings["max_tokens"] = int(max_tokens)
        except ValueError:
            if ui:
                ui.print_warning("Invalid max tokens value. Using default.")
            else:
                print("Invalid max tokens value. Using default.")
    
    if temperature:
        try:
            temp = float(temperature)
            if 0 <= temp <= 1:
                settings["temperature"] = temp
            else:
                if ui:
                    ui.print_warning("Temperature must be between 0 and 1. Using default.")
                else:
                    print("Temperature must be between 0 and 1. Using default.")
        except ValueError:
            if ui:
                ui.print_warning("Invalid temperature value. Using default.")
            else:
                print("Invalid temperature value. Using default.")
    
    if top_p:
        try:
            p = float(top_p)
            if 0 <= p <= 1:
                settings["top_p"] = p
            else:
                if ui:
                    ui.print_warning("Top_p must be between 0 and 1. Using default.")
                else:
                    print("Top_p must be between 0 and 1. Using default.")
        except ValueError:
            if ui:
                ui.print_warning("Invalid top_p value. Using default.")
            else:
                print("Invalid top_p value. Using default.")
    
    if ui:
        if monitor_resources:
            settings["monitor_resources"] = True
    else:
        if monitor_resources.startswith('y'):
            settings["monitor_resources"] = True
    
    return settings

#===============================================================================
# Main Execution
#===============================================================================

if __name__ == "__main__":
    if IN_COLAB:
        # Setup signal handler for graceful shutdown
        def signal_handler(sig, frame):
            print("\n\n👋 Shutting down Ollama...")
            !pkill ollama
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        
        # Initialize the enhanced UI
        ui = ConsoleUI(app_name="DeepSeek Chat", theme="blue")
        ui.display_splash()
        
        # Create directories
        setup_directories()
        
        # Install and run Ollama
        if install_ollama(ui):
            # List available models
            available_models = list_available_models(ui)
            
            if not available_models:
                # Let user select and pull a model
                ui.print_warning("No models found. Let's pull a DeepSeek model.")
                model_name = pull_deepseek_model(ui)
            else:
                # Select model or pull a new one
                options = ["Use an existing model", "Pull a new DeepSeek model"]
                choice_idx, _ = ui.prompt_choice("Found existing models. What would you like to do?", options)
                
                if choice_idx == 2:
                    model_name = pull_deepseek_model(ui)
                else:
                    # Select from available models
                    choice_idx, model_name = ui.prompt_choice("Select a model to use:", available_models)
            
            if model_name:
                ui.print_success(f"Setup complete! {model_name} is ready to use.")
                
                # Main menu loop
                while True:
                    ui.print_section_header("Main Menu")
                    
                    options = [
                        "Interactive Chat",
                        "Load Previous Conversation",
                        "Benchmark Models",
                        "Compare Models",
                        "Batch Processing",
                        "Web UI (Gradio)",
                        "Change UI Theme",
                        "Exit"
                    ]
                    
                    choice_idx, choice = ui.prompt_choice("Select an option:", options)
                    
                    if choice_idx == 1:  # Interactive Chat
                        # Get initial prompt if user wants to provide one
                        initial_prompt = ui.prompt_input("Enter an initial prompt (or press Enter to skip)")
                        
                        # Options for the user
                        sub_options = ["Start chat with default settings", "Configure advanced settings"]
                        sub_idx, _ = ui.prompt_choice("Choose an option:", sub_options)
                        
                        if sub_idx == 2:
                            settings = advanced_settings(ui)
                            settings["model_name"] = model_name
                            if initial_prompt:
                                settings["initial_prompt"] = initial_prompt
                            interactive_chat(model_name=model_name, ui=ui, **settings)
                        else:
                            interactive_chat(model_name=model_name, ui=ui, initial_prompt=initial_prompt)
                    
                    elif choice_idx == 2:  # Load Previous Conversation
                        # Load previous conversation
                        chat_data = load_chat_history(ui)
                        if chat_data:
                            ui.print_success(f"Resuming conversation with model: {chat_data['model']}")
                            interactive_chat(
                                model_name=chat_data['model'],
                                ui=ui,
                                chat_history=chat_data['history']
                            )
                    
                    elif choice_idx == 3:  # Benchmark Models
                        benchmark_models(ui)
                    
                    elif choice_idx == 4:  # Compare Models
                        compare_models(ui)
                    
                    elif choice_idx == 5:  # Batch Processing
                        batch_processing(ui)
                    
                    elif choice_idx == 6:  # Web UI (Gradio)
                        create_web_ui(model_name, ui)
                    
                    elif choice_idx == 7:  # Change UI Theme
                        themes = ["Blue", "Green", "Purple", "Dark"]
                        theme_idx, theme_name = ui.prompt_choice("Choose a new theme:", themes)
                        ui.set_theme(theme_name.lower())
                        ui.print_success(f"Theme changed to {theme_name}")
                        
                    elif choice_idx == 8:  # Exit
                        ui.print_info("Exiting program.")
                        !pkill ollama
                        break
            else:
                ui.print_error("Failed to set up a model.")
        else:
            ui.print_error("Failed to set up Ollama.")
            
        # Show example API usage
        ui.print_section_header("Example API Usage")
        ui.print_info("You can copy and modify this code for your own applications:")
        
        example_code = """
# Simple API call to DeepSeek model
response = query_deepseek(
    prompt="Explain quantum computing in simple terms.",
    model="deepseek-llm:7b",  # Use the model name you selected
    stream=False,
    system_prompt="You are a helpful assistant that explains complex topics simply.",
    max_tokens=1024,
    temperature=0.7
)
print(response)
        """
        
        # Print code with syntax highlighting
        for line in example_code.strip().split('\n'):
            print(f"{ui.theme['accent']}{line}{ui.theme['reset']}")
    else:
        print("❌ This script is designed to run in Google Colab environment.")
