#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Enhanced DeepSeek Models with Ollama on Google Colab
===================================================
This script sets up and runs DeepSeek models using Ollama in a Google Colab environment.
It includes installation of Ollama, downloading the selected DeepSeek model, and a versatile
interface for interacting with the model with extended features.

Features:
- Ollama installation and model management
- Interactive chat with DeepSeek models
- Save and load conversation history
- Benchmark different models and settings
- GPU/Memory monitoring
- File input processing for context
- Multi-model comparison
- Web UI option with Gradio
- Batch processing of prompts

Author: Senior DeepSeek AI Scientist
Date: April 15, 2025
"""

import os
import time
import requests
import subprocess
import IPython.display
from IPython.display import HTML, display
import json
import sys
import signal
import datetime
import psutil
import threading
import re
import glob
import csv
import matplotlib.pyplot as plt
import pandas as pd
from tqdm.auto import tqdm

# Try importing optional libraries
try:
    import gradio as gr
    GRADIO_AVAILABLE = True
except ImportError:
    GRADIO_AVAILABLE = False

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# Display styled header
display(HTML("""
<div style="background-color: #f8f9fa; padding: 10px; border-radius: 10px; margin-bottom: 15px;">
    <h1 style="text-align: center; color: #4285f4;">Enhanced DeepSeek Models with Ollama on Google Colab</h1>
    <p style="text-align: center; font-style: italic;">Set up and run DeepSeek models locally using Ollama with extended features</p>
</div>
"""))

# Check if running in Google Colab
try:
    import google.colab
    IN_COLAB = True
    print("‚úÖ Running in Google Colab environment")
except ImportError:
    IN_COLAB = False
    print("‚ö†Ô∏è Not running in Google Colab environment. Some features might not work correctly.")

# Create directories for saving data
def setup_directories():
    """Create necessary directories for saving chat history, models info, etc."""
    dirs = ['./chat_history', './benchmarks', './model_configs', './input_files']
    for d in dirs:
        if not os.path.exists(d):
            os.makedirs(d)
            print(f"üìÅ Created directory: {d}")

# Install Ollama
print("\nüöÄ Setting up Ollama...")

def install_ollama():
    """Download and install Ollama, then start the server."""
    print("üì¶ Installing Ollama...")
    
    # Install necessary dependencies
    if IN_COLAB:
        try:
            !pip install -q psutil matplotlib pandas tqdm
            print("‚úÖ Installed supporting libraries")
            
            # Optional installs
            try:
                !pip install -q gradio
                print("‚úÖ Installed Gradio for web UI")
            except:
                print("‚ö†Ô∏è Failed to install Gradio. Web UI will not be available.")
        except:
            print("‚ö†Ô∏è Failed to install some supporting libraries. Some features may not work.")
    
    # Download and install Ollama
    !curl -fsSL https://ollama.com/install.sh | sh
    
    # Start Ollama server
    !ollama serve > /dev/null 2>&1 &
    
    # Wait for Ollama to start
    print("üîÑ Starting Ollama server...")
    time.sleep(5)
    
    # Ensure Ollama is running
    try:
        response = requests.get("http://localhost:11434/api/version")
        if response.status_code == 200:
            version = response.json().get("version", "unknown")
            print(f"‚úÖ Ollama {version} is running successfully!")
            return True
    except:
        print("‚ùå Failed to start Ollama server.")
        return False

# List available models
def list_available_models(keyword=None):
    """List available models in Ollama, with optional filtering by keyword."""
    print("\nüìã Listing available models in Ollama...")
    try:
        if keyword:
            !ollama list | grep -i {keyword}
            print(f"\nüîç Filtered models by: {keyword}")
        else:
            !ollama list
        print("\nüåê If you don't see your desired models, you can check available models at: https://ollama.com/library")
        
        # Return models as a list
        result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
        models = []
        for line in result.stdout.strip().split('\n')[1:]:  # Skip header
            if line.strip():
                model_name = line.split()[0]
                models.append(model_name)
        return models
    except Exception as e:
        print(f"‚ùå Failed to list models: {e}")
        return []

# Pull DeepSeek model
def pull_deepseek_model():
    """Allow user to select and pull a DeepSeek model."""
    # List common DeepSeek model options
    deepseek_options = [
        "deepseek-r1:latest",
        "deepseek-coder:latest",
        "deepseek-coder:6.7b",
        "deepseek-coder:33b",
        "deepseek-llm:latest", 
        "deepseek-llm:7b",
        "deepseek-llm:67b"
    ]
    
    print("\nüîç Common DeepSeek model options:")
    for i, model in enumerate(deepseek_options, 1):
        print(f"{i}. {model}")
    print(f"{len(deepseek_options) + 1}. Custom model name")
    print(f"{len(deepseek_options) + 2}. Search available models")
    
    # Ask user to select a model
    while True:
        choice = input("\nEnter your choice (number): ").strip()
        try:
            choice_num = int(choice)
            if 1 <= choice_num <= len(deepseek_options):
                model_name = deepseek_options[choice_num - 1]
                break
            elif choice_num == len(deepseek_options) + 1:
                model_name = input("Enter custom DeepSeek model name: ").strip()
                break
            elif choice_num == len(deepseek_options) + 2:
                keyword = input("Enter search term for models: ").strip()
                list_available_models(keyword)
                continue
            else:
                print("Invalid choice. Please try again.")
        except ValueError:
            print("Please enter a valid number.")
    
    print(f"\nüì• Pulling DeepSeek model: {model_name} (this may take some time)...")
    !ollama pull {model_name}
    
    # Verify model is available
    try:
        response = requests.get("http://localhost:11434/api/tags")
        models = [model['name'] for model in response.json().get('models', [])]
        if model_name in models:
            print(f"‚úÖ {model_name} model has been successfully pulled!")
            return model_name
        else:
            print(f"‚ö†Ô∏è Verification failed. Let's check if the model is available:")
            !ollama list | grep -i {model_name.split(':')[0]}
            confirm = input("\nDid you see your model in the list above? (yes/no): ").lower()
            if confirm.startswith('y'):
                return model_name
            else:
                print(f"‚ùå {model_name} model not found after pulling.")
                return None
    except:
        print("‚ùå Failed to verify model.")
        # Try to list models anyway
        !ollama list
        confirm = input("\nDid you see your model in the list above? (yes/no): ").lower()
        if confirm.startswith('y'):
            return model_name
        return None

# Function to interact with DeepSeek model
def query_deepseek(prompt, model, stream=True, system_prompt=None, max_tokens=2048, temperature=0.7, 
                   top_p=0.9, context=None, format=None, num_predict=None):
    """Query a DeepSeek model with extended parameters."""
    url = "http://localhost:11434/api/generate"
    
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": stream,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": top_p,
    }
    
    if system_prompt:
        payload["system"] = system_prompt
    
    if context:
        payload["context"] = context
    
    if format:
        payload["format"] = format
    
    if num_predict:
        payload["num_predict"] = num_predict
    
    if stream:
        response = requests.post(url, json=payload, stream=True)
        response_text = ""
        
        for line in response.iter_lines():
            if line:
                try:
                    chunk = json.loads(line)
                    if 'response' in chunk:
                        response_text += chunk['response']
                        # Print response token by token
                        print(chunk['response'], end='', flush=True)
                    if chunk.get('done', False):
                        break
                except json.JSONDecodeError:
                    print(f"Failed to decode JSON: {line}")
        
        print("\n")  # Add newline after completion
        return response_text
    else:
        start_time = time.time()
        response = requests.post(url, json=payload)
        end_time = time.time()
        
        if response.status_code == 200:
            result = response.json()
            # Add timing information
            result['elapsed_time'] = end_time - start_time
            return result
        else:
            print(f"Error: {response.status_code}, {response.text}")
            return {"error": response.text, "status_code": response.status_code}

# Load text file content
def load_file_content(file_path):
    """Load content from a text file to use as context."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        print(f"‚úÖ Loaded file: {file_path} ({len(content)} characters)")
        return content
    except Exception as e:
        print(f"‚ùå Error loading file {file_path}: {e}")
        return None

# Function to upload a file
def upload_file():
    """Upload a file for processing in Colab."""
    if IN_COLAB:
        from google.colab import files
        try:
            uploaded = files.upload()
            for filename in uploaded.keys():
                # Save to input_files directory
                with open(f"./input_files/{filename}", "wb") as f:
                    f.write(uploaded[filename])
                print(f"‚úÖ Saved uploaded file to ./input_files/{filename}")
                return f"./input_files/{filename}"
        except Exception as e:
            print(f"‚ùå Error during file upload: {e}")
            return None
    else:
        print("‚ö†Ô∏è File upload is only available in Google Colab.")
        file_path = input("Enter path to file: ").strip()
        if os.path.exists(file_path):
            return file_path
        else:
            print("‚ùå File not found.")
            return None

# Save chat history to file
def save_chat_history(chat_history, model_name):
    """Save chat history to a JSON file."""
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"./chat_history/chat_{model_name.replace(':', '_')}_{timestamp}.json"
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                "model": model_name,
                "timestamp": timestamp,
                "history": chat_history
            }, f, indent=2)
        print(f"‚úÖ Chat history saved to: {filename}")
        return filename
    except Exception as e:
        print(f"‚ùå Error saving chat history: {e}")
        return None

# Load chat history from file
def load_chat_history():
    """Load a previously saved chat history."""
    history_files = glob.glob("./chat_history/*.json")
    
    if not history_files:
        print("‚ùå No saved chat history found.")
        return None
    
    print("\nüìã Available chat history files:")
    for i, filepath in enumerate(history_files, 1):
        filename = os.path.basename(filepath)
        print(f"{i}. {filename}")
    
    while True:
        choice = input("\nEnter your choice (number) or 0 to cancel: ").strip()
        try:
            choice_num = int(choice)
            if choice_num == 0:
                return None
            if 1 <= choice_num <= len(history_files):
                filepath = history_files[choice_num - 1]
                
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                print(f"‚úÖ Loaded chat history with model: {data['model']}")
                return data
            else:
                print("Invalid choice. Please try again.")
        except ValueError:
            print("Please enter a valid number.")
        except Exception as e:
            print(f"‚ùå Error loading chat history: {e}")
            return None

# Monitor system resources
class ResourceMonitor:
    """Monitor CPU, RAM, and GPU (if available) usage."""
    def __init__(self, interval=1.0):
        self.interval = interval
        self.running = False
        self.data = {
            'time': [],
            'cpu': [],
            'ram': [],
            'gpu': []
        }
    
    def start(self):
        """Start monitoring resources."""
        self.running = True
        self.data = {'time': [], 'cpu': [], 'ram': [], 'gpu': []}
        self.thread = threading.Thread(target=self._monitor)
        self.thread.daemon = True
        self.thread.start()
        print("üìä Resource monitoring started")
    
    def stop(self):
        """Stop monitoring resources."""
        self.running = False
        if hasattr(self, 'thread'):
            self.thread.join(timeout=2.0)
        print("üìä Resource monitoring stopped")
        return self.data
    
    def _monitor(self):
        """Internal monitoring function."""
        start_time = time.time()
        while self.running:
            current_time = time.time() - start_time
            
            # CPU and RAM
            cpu_percent = psutil.cpu_percent()
            ram_percent = psutil.virtual_memory().percent
            
            # GPU if available
            gpu_percent = None
            if TORCH_AVAILABLE and torch.cuda.is_available():
                try:
                    gpu_percent = torch.cuda.utilization()
                except:
                    try:
                        # Alternative method
                        result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                                                capture_output=True, text=True)
                        gpu_percent = float(result.stdout.strip())
                    except:
                        gpu_percent = None
            
            # Save data
            self.data['time'].append(current_time)
            self.data['cpu'].append(cpu_percent)
            self.data['ram'].append(ram_percent)
            self.data['gpu'].append(gpu_percent)
            
            time.sleep(self.interval)
    
    def plot(self):
        """Plot the collected resource usage data."""
        if not self.data['time']:
            print("‚ùå No monitoring data available to plot")
            return
        
        fig, ax1 = plt.subplots(figsize=(10, 6))
        
        # CPU and RAM
        ax1.set_xlabel('Time (seconds)')
        ax1.set_ylabel('Usage (%)')
        ax1.plot(self.data['time'], self.data['cpu'], 'b-', label='CPU')
        ax1.plot(self.data['time'], self.data['ram'], 'g-', label='RAM')
        
        # GPU if available
        if any(x is not None for x in self.data['gpu']):
            ax1.plot(self.data['time'], self.data['gpu'], 'r-', label='GPU')
        
        ax1.set_ylim(0, 100)
        ax1.legend(loc='upper left')
        ax1.grid(True)
        
        plt.title('System Resource Usage')
        plt.tight_layout()
        plt.show()

# Interactive chat with DeepSeek model
def interactive_chat(model_name, system_prompt=None, initial_prompt=None, chat_history=None, 
                     max_tokens=2048, temperature=0.7, monitor_resources=False):
    """Interactive chat with a model with more features."""
    print(f"\nü§ñ Starting interactive chat with {model_name}...")
    print("üîç Type 'exit', 'quit', or press Ctrl+C to end the conversation")
    print("üìù Special commands:")
    print("  - /save : Save the chat history")
    print("  - /file : Upload and use a file as context")
    print("  - /reset : Reset the conversation")
    print("  - /system <prompt> : Change the system prompt")
    print("  - /help : Show available commands")
    
    if system_prompt:
        print(f"\nüîß Using system prompt: \"{system_prompt}\"")
    
    # Initialize or use provided chat history
    if chat_history is None:
        chat_history = []
    
    # Resource monitoring
    monitor = None
    if monitor_resources:
        monitor = ResourceMonitor()
        monitor.start()
    
    # Context for file input
    file_context = None
    
    try:
        # If an initial prompt was provided, use it first
        if initial_prompt:
            print(f"\nüßë Initial Prompt: {initial_prompt}")
            chat_history.append({"role": "user", "content": initial_prompt})
            
            print(f"\nü§ñ {model_name}: ", end="")
            
            response = query_deepseek(
                initial_prompt, 
                model=model_name,
                system_prompt=system_prompt,
                stream=True,
                max_tokens=max_tokens,
                temperature=temperature,
                context=file_context
            )
            
            chat_history.append({"role": "assistant", "content": response})
        
        # Continue with interactive chat
        while True:
            user_input = input("\nüßë You: ")
            
            # Check for special commands
            if user_input.lower() in ['exit', 'quit']:
                print("\nüëã Ending conversation.")
                break
            
            elif user_input.lower() == '/save':
                save_path = save_chat_history(chat_history, model_name)
                continue
            
            elif user_input.lower() == '/file':
                file_path = upload_file()
                if file_path:
                    file_context = load_file_content(file_path)
                    print(f"‚úÖ File content loaded as context. ({len(file_context)} characters)")
                continue
            
            elif user_input.lower() == '/reset':
                chat_history = []
                file_context = None
                print("üîÑ Conversation reset.")
                continue
            
            elif user_input.lower().startswith('/system '):
                system_prompt = user_input[8:].strip()
                print(f"üîß System prompt updated: \"{system_prompt}\"")
                continue
            
            elif user_input.lower() == '/help':
                print("\nüìù Available commands:")
                print("  - /save : Save the chat history")
                print("  - /file : Upload and use a file as context")
                print("  - /reset : Reset the conversation")
                print("  - /system <prompt> : Change the system prompt")
                print("  - /help : Show this help message")
                print("  - exit, quit : End the conversation")
                continue
            
            chat_history.append({"role": "user", "content": user_input})
            
            print(f"\nü§ñ {model_name}: ", end="")
            
            response = query_deepseek(
                user_input, 
                model=model_name,
                system_prompt=system_prompt,
                stream=True,
                max_tokens=max_tokens,
                temperature=temperature,
                context=file_context
            )
            
            chat_history.append({"role": "assistant", "content": response})
    
    except KeyboardInterrupt:
        print("\n\nüëã Conversation interrupted.")
    
    finally:
        # Stop resource monitoring if active
        if monitor_resources and monitor:
            data = monitor.stop()
            save_monitoring = input("\nDo you want to see the resource usage graph? (yes/no): ").lower()
            if save_monitoring.startswith('y'):
                monitor.plot()
        
        # Offer to save chat history
        save_option = input("\nDo you want to save this chat history? (yes/no): ").lower()
        if save_option.startswith('y'):
            save_path = save_chat_history(chat_history, model_name)
        
        print("\nüëã Chat session ended.")

# Benchmark models or parameters
def benchmark_models():
    """Benchmark different models or parameter settings."""
    print("\nüìä Model Benchmarking Tool")
    
    # Select models to benchmark
    available_models = list_available_models()
    
    if not available_models:
        print("‚ùå No models available for benchmarking.")
        return
    
    # Select models to benchmark
    print("\nSelect models to benchmark:")
    for i, model in enumerate(available_models, 1):
        print(f"{i}. {model}")
    
    model_choices = input("\nEnter model numbers to benchmark (comma-separated) or 'all': ").strip()
    
    benchmark_models = []
    if model_choices.lower() == 'all':
        benchmark_models = available_models
    else:
        try:
            indices = [int(idx.strip()) - 1 for idx in model_choices.split(',')]
            benchmark_models = [available_models[idx] for idx in indices if 0 <= idx < len(available_models)]
        except:
            print("‚ùå Invalid selection. Using first model only.")
            benchmark_models = [available_models[0]]
    
    print(f"\n‚úÖ Models to benchmark: {', '.join(benchmark_models)}")
    
    # Get benchmark parameters
    print("\nüìù Enter benchmark parameters:")
    prompts = []
    
    # Get prompts
    prompt_source = input("Enter prompt source (1. Input prompts, 2. Load from file): ").strip()
    
    if prompt_source == '2':
        file_path = upload_file()
        if file_path:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    prompts = [line.strip() for line in f if line.strip()]
                print(f"‚úÖ Loaded {len(prompts)} prompts from file")
            except Exception as e:
                print(f"‚ùå Error loading prompts: {e}")
                return
    else:
        num_prompts = input("Enter number of prompts to benchmark: ").strip()
        try:
            num_prompts = int(num_prompts)
        except:
            num_prompts = 1
        
        for i in range(num_prompts):
            prompt = input(f"Enter prompt {i+1}/{num_prompts}: ").strip()
            prompts.append(prompt)
    
    if not prompts:
        print("‚ùå No prompts provided for benchmarking.")
        return
    
    # Get system prompt
    system_prompt = input("Enter system prompt (or leave empty): ").strip()
    
    # Get max tokens
    max_tokens = input("Enter max tokens (default: 1024): ").strip()
    try:
        max_tokens = int(max_tokens)
    except:
        max_tokens = 1024
    
    # Get temperature
    temperature = input("Enter temperature (default: 0.7): ").strip()
    try:
        temperature = float(temperature)
    except:
        temperature = 0.7
    
    # Number of runs per model
    runs = input("Enter number of runs per prompt (default: 1): ").strip()
    try:
        runs = int(runs)
    except:
        runs = 1
    
    # Prepare results
    results = []
    
    # Run benchmarks
    print(f"\nüöÄ Running benchmarks ({len(benchmark_models)} models √ó {len(prompts)} prompts √ó {runs} runs)...")
    
    progress_total = len(benchmark_models) * len(prompts) * runs
    progress_bar = tqdm(total=progress_total, desc="Benchmarking")
    
    for model_name in benchmark_models:
        for prompt in prompts:
            for run in range(runs):
                # Query model with stream=False to get timing info
                result = query_deepseek(
                    prompt=prompt[:50] + "..." if len(prompt) > 50 else prompt,
                    model=model_name,
                    stream=False,
                    system_prompt=system_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
                if 'error' not in result:
                    # Extract results
                    results.append({
                        'model': model_name,
                        'prompt': prompt[:50] + "..." if len(prompt) > 50 else prompt,
                        'run': run + 1,
                        'time': result.get('elapsed_time', 0),
                        'tokens': len(result.get('response', '').split())
                    })
                
                progress_bar.update(1)
                # Small delay to avoid overwhelming the server
                time.sleep(0.5)
    
    progress_bar.close()
    
    # Show results
    if results:
        # Convert to DataFrame for analysis
        df = pd.DataFrame(results)
        
        # Calculate average time and tokens per model
        model_stats = df.groupby('model').agg({
            'time': ['mean', 'std'],
            'tokens': ['mean', 'std']
        })
        
        print("\nüìä Benchmark Results:")
        print(model_stats)
        
        # Plot results
        plt.figure(figsize=(12, 6))
        
        plt.subplot(1, 2, 1)
        df.groupby('model')['time'].mean().plot(kind='bar')
        plt.title('Average Response Time by Model')
        plt.ylabel('Time (seconds)')
        plt.xticks(rotation=45)
        
        plt.subplot(1, 2, 2)
        df.groupby('model')['tokens'].mean().plot(kind='bar')
        plt.title('Average Tokens by Model')
        plt.ylabel('Tokens')
        plt.xticks(rotation=45)
        
        plt.tight_layout()
        plt.show()
        
        # Save results
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = f"./benchmarks/benchmark_{timestamp}.csv"
        df.to_csv(result_file, index=False)
        print(f"\n‚úÖ Benchmark results saved to {result_file}")
    else:
        print("‚ùå No valid benchmark results collected.")

# Multi-model comparison
def compare_models():
    """Compare multiple models with the same prompt."""
    print("\nüîç Model Comparison Tool")
    
    # Get available models
    available_models = list_available_models()
    
    if not available_models:
        print("‚ùå No models available for comparison.")
        return
    
    # Select models to compare
    print("\nSelect models to compare:")
    for i, model in enumerate(available_models, 1):
        print(f"{i}. {model}")
    
    model_choices = input("\nEnter model numbers to compare (comma-separated, max 4): ").strip()
    
    compare_models_list = []
    try:
        indices = [int(idx.strip()) - 1 for idx in model_choices.split(',')]
        compare_models_list = [available_models[idx] for idx in indices if 0 <= idx < len(available_models)]
        # Limit to 4 models for comparison
        compare_models_list = compare_models_list[:4]
    except:
        print("‚ùå Invalid selection. Using first model only.")
        compare_models_list = [available_models[0]]
    
    if len(compare_models_list) < 2:
        print("‚ö†Ô∏è Need at least 2 models for comparison. Adding another model.")
        if len(available_models) > 1:
            compare_models_list.append(available_models[1 if 0 in indices else 0])
    
    print(f"\n‚úÖ Models to compare: {', '.join(compare_models_list)}")
    
    # Get the prompt to use for comparison
    prompt = input("\nEnter prompt for comparison: ").strip()
    if not prompt:
        prompt = "Explain the advantages and applications of large language models in three paragraphs."
        print(f"Using default prompt: \"{prompt}\"")
    
    # Get system prompt
    system_prompt = input("Enter system prompt (or leave empty): ").strip()
    
    # Get max tokens
    max_tokens = input("Enter max tokens (default: 1024): ").strip()
    try:
        max_tokens = int(max_tokens)
    except:
        max_tokens = 1024
    
    # Get temperature
    temperature = input("Enter temperature (default: 0.7): ").strip()
    try:
        temperature = float(temperature)
    except:
        temperature = 0.7
    
    # Run comparison
    print("\nüîÑ Running model comparison...")
    
    results = {}
    
    for model_name in compare_models_list:
        print(f"\nü§ñ Model: {model_name}")
        print("Generating response...", end="", flush=True)
        
        start_time = time.time()
        response = query_deepseek(
            prompt=prompt,
            model=model_name,
            stream=False,
            system_prompt=system_prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )
        elapsed_time = time.time() - start_time
        
        print(" Done!")
        
        if 'error' not in response:
            # Store results
            results[model_name] = {
                'response': response.get('response', ''),
                'time': elapsed_time,
                'tokens': len(response.get('response', '').split())
            }
    
    # Display results
    if results:
        print("\nüìä Comparison Results:")
        
        # Show time and token statistics
        print("\n‚è±Ô∏è Response Times:")
        for model, data in results.items():
            print(f"  - {model}: {data['time']:.2f} seconds ({data['tokens']} tokens)")
        
        # Show responses side-by-side in a grid
        print("\nüîç Model Responses:")
        for i, model in enumerate(results.keys(), 1):
            print(f"\nüìù Model {i}: {model}")
            print("-" * 80)
            print(results[model]['response'])
            print("-" * 80)
        
        # Save comparison
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        comparison_file = f"./benchmarks/comparison_{timestamp}.json"
        
        with open(comparison_file, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': timestamp,
                'prompt': prompt,
                'system_prompt': system_prompt,
                'max_tokens': max_tokens,
                'temperature': temperature,
                'results': results
            }, f, indent=2)
        
        print(f"\n‚úÖ Comparison results saved to {comparison_file}")
    else:
        print("‚ùå No valid comparison results collected.")

# Batch processing of prompts
def batch_processing():
    """Process multiple prompts in batch mode."""
    print("\nüìö Batch Processing Tool")
    
    # Select a model
    available_models = list_available_models()
    
    if not available_models:
        print("‚ùå No models available for batch processing.")
        return
    
    # Select model
    print("\nSelect a model for batch processing:")
    for i, model in enumerate(available_models, 1):
        print(f"{i}. {model}")
    
    model_choice = input("\nEnter model number: ").strip()
    try:
        model_idx = int(model_choice) - 1
        if 0 <= model_idx < len(available_models):
            model_name = available_models[model_idx]
        else:
            print("‚ùå Invalid selection. Using first model.")
            model_name = available_models[0]
    except:
        print("‚ùå Invalid selection. Using first model.")
        model_name = available_models[0]
    
    print(f"\n‚úÖ Using model: {model_name}")
    
    # Get prompts
    print("\nSelect prompt source:")
    print("1. Enter prompts manually")
    print("2. Upload a CSV file with prompts")
    print("3. Upload a text file with one prompt per line")
    
    source_choice = input("\nEnter your choice (1-3): ").strip()
    
    prompts = []
    
    if source_choice == '1':
        num_prompts = input("Enter number of prompts: ").strip()
        try:
            num_prompts = int(num_prompts)
        except:
            num_prompts = 1
        
        for i in range(num_prompts):
            prompt = input(f"Enter prompt {i+1}/{num_prompts}: ").strip()
            prompts.append(prompt)
    
    elif source_choice == '2':
        file_path = upload_file()
        if file_path:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    csv_reader = csv.reader(f)
                    header = next(csv_reader, None)
                    
                    if header:
                        print("\nCSV columns found:")
                        for i, col in enumerate(header):
                            print(f"{i+1}. {col}")
                        
                        col_choice = input("Which column contains the prompts? (number): ").strip()
                        try:
                            col_idx = int(col_choice) - 1
                        except:
                            col_idx = 0
                        
                        # Read prompts from selected column
                        for row in csv_reader:
                            if 0 <= col_idx < len(row):
                                prompts.append(row[col_idx])
                    else:
                        # No header, use first column
                        f.seek(0)
                        for row in csv_reader:
                            if row:
                                prompts.append(row[0])
                
                print(f"‚úÖ Loaded {len(prompts)} prompts from CSV")
            except Exception as e:
                print(f"‚ùå Error loading CSV: {e}")
                return
    
    elif source_choice == '3':
        file_path = upload_file()
        if file_path:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    prompts = [line.strip() for line in f if line.strip()]
                print(f"‚úÖ Loaded {len(prompts)} prompts from text file")
            except Exception as e:
                print(f"‚ùå Error loading text file: {e}")
                return
    
    if not prompts:
        print("‚ùå No prompts provided for batch processing.")
        return
    
    # Get system prompt
    system_prompt = input("Enter system prompt (or leave empty): ").strip()
    
    # Get max tokens
    max_tokens = input("Enter max tokens (default: 1024): ").strip()
    try:
        max_tokens = int(max_tokens)
    except:
        max_tokens = 1024
    
    # Get temperature
    temperature = input("Enter temperature (default: 0.7): ").strip()
    try:
        temperature = float(temperature)
    except:
        temperature = 0.7
    
    # Process prompts
    print(f"\nüîÑ Processing {len(prompts)} prompts in batch mode...")
    
    results = []
    progress_bar = tqdm(total=len(prompts), desc="Processing")
    
    for i, prompt in enumerate(prompts):
        try:
            response = query_deepseek(
                prompt=prompt,
                model=model_name,
                stream=False,
                system_prompt=system_prompt,
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            if 'error' not in response:
                results.append({
                    'prompt_id': i + 1,
                    'prompt': prompt,
                    'response': response.get('response', ''),
                    'time': response.get('elapsed_time', 0)
                })
            else:
                results.append({
                    'prompt_id': i + 1,
                    'prompt': prompt,
                    'response': f"ERROR: {response.get('error', 'Unknown error')}",
                    'time': 0
                })
        except Exception as e:
            results.append({
                'prompt_id': i + 1,
                'prompt': prompt,
                'response': f"ERROR: {str(e)}",
                'time': 0
            })
        
        progress_bar.update(1)
        # Small delay to avoid overwhelming the server
        time.sleep(0.5)
    
    progress_bar.close()
    
    # Save results
    if results:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"./benchmarks/batch_{timestamp}.csv"
        
        with open(output_file, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['prompt_id', 'prompt', 'response', 'time'])
            
            for result in results:
                writer.writerow([
                    result['prompt_id'],
                    result['prompt'],
                    result['response'],
                    result['time']
                ])
        
        print(f"\n‚úÖ Batch processing results saved to {output_file}")
        
        # Display summary
        total_time = sum(result['time'] for result in results)
        avg_time = total_time / len(results) if results else 0
        
        print(f"\nüìä Batch Processing Summary:")
        print(f"  - Total prompts: {len(prompts)}")
        print(f"  - Successful responses: {sum(1 for r in results if not r['response'].startswith('ERROR'))}")
        print(f"  - Failed responses: {sum(1 for r in results if r['response'].startswith('ERROR'))}")
        print(f"  - Total processing time: {total_time:.2f} seconds")
        print(f"  - Average time per prompt: {avg_time:.2f} seconds")
    else:
        print("‚ùå No results collected during batch processing.")

# Web UI with Gradio
def create_web_ui(model_name):
    """Create a simple web UI using Gradio."""
    if not GRADIO_AVAILABLE:
        print("‚ùå Gradio is not available. Cannot create web UI.")
        print("üìù Try running: !pip install gradio")
        return
    
    print(f"\nüåê Creating web UI for model: {model_name}")
    
    # Define system prompt
    system_prompt_input = gr.Textbox(
        label="System Prompt",
        placeholder="You are a helpful AI assistant...",
        lines=2
    )
    
    # Define chat interface
    def respond(message, history, system_prompt):
        # Format prompt from history
        if not history:
            prompt = message
        else:
            prompt = message
        
        # Get response
        response = query_deepseek(
            prompt=message,
            model=model_name,
            stream=False,
            system_prompt=system_prompt
        )
        
        if 'error' in response:
            return f"Error: {response.get('error', 'Unknown error')}"
        
        return response.get('response', '')
    
    # Create interface
    chat_interface = gr.ChatInterface(
        fn=lambda message, history, system_prompt: respond(message, history, system_prompt),
        additional_inputs=[system_prompt_input],
        title=f"DeepSeek {model_name} Chat",
        description=f"Chat with the DeepSeek {model_name} model using Ollama",
        examples=[
            ["Tell me about large language models"],
            ["Write a Python function to calculate Fibonacci numbers"],
            ["Explain quantum computing in simple terms"],
        ],
        theme="default"
    )
    
    # Launch interface
    chat_interface.launch(share=True, inbrowser=True)

# Function to set advanced parameters
def advanced_settings():
    """Configure advanced model settings."""
    print("\n‚öôÔ∏è Advanced Settings:")
    system_prompt = input("Enter system prompt (leave empty for default): ").strip()
    max_tokens = input("Enter max tokens (default: 2048): ").strip()
    temperature = input("Enter temperature (0-1, default: 0.7): ").strip()
    top_p = input("Enter top_p (0-1, default: 0.9): ").strip()
    monitor_resources = input("Monitor system resources? (yes/no, default: no): ").strip().lower()
    
    settings = {}
    
    if system_prompt:
        settings["system_prompt"] = system_prompt
    
    if max_tokens:
        try:
            settings["max_tokens"] = int(max_tokens)
        except ValueError:
            print("Invalid max tokens value. Using default.")
    
    if temperature:
        try:
            temp = float(temperature)
            if 0 <= temp <= 1:
                settings["temperature"] = temp
            else:
                print("Temperature must be between 0 and 1. Using default.")
        except ValueError:
            print("Invalid temperature value. Using default.")
    
    if top_p:
        try:
            p = float(top_p)
            if 0 <= p <= 1:
                settings["top_p"] = p
            else:
                print("Top_p must be between 0 and 1. Using default.")
        except ValueError:
            print("Invalid top_p value. Using default.")
    
    if monitor_resources.startswith('y'):
        settings["monitor_resources"] = True
    
    return settings

# Main execution
if __name__ == "__main__":
    if IN_COLAB:
        # Setup signal handler for graceful shutdown
        def signal_handler(sig, frame):
            print("\n\nüëã Shutting down Ollama...")
            !pkill ollama
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        
        # Create directories
        setup_directories()
        
        # Install and run Ollama
        if install_ollama():
            # List available models
            available_models = list_available_models()
            
            if not available_models:
                # Let user select and pull a model
                model_name = pull_deepseek_model()
            else:
                print("\nüîç Found existing models. Do you want to:")
                print("1. Use an existing model")
                print("2. Pull a new DeepSeek model")
                
                choice = input("Enter your choice (1 or 2): ").strip()
                
                if choice == "2":
                    model_name = pull_deepseek_model()
                else:
                    # Select from available models
                    print("\nüîç Select a model:")
                    for i, model in enumerate(available_models, 1):
                        print(f"{i}. {model}")
                    
                    choice = input("\nEnter your choice (number): ").strip()
                    try:
                        choice_num = int(choice)
                        if 1 <= choice_num <= len(available_models):
                            model_name = available_models[choice_num - 1]
                        else:
                            print("Invalid choice. Using first model.")
                            model_name = available_models[0]
                    except:
                        print("Invalid choice. Using first model.")
                        model_name = available_models[0]
            
            if model_name:
                print(f"\nüéâ Setup complete! {model_name} is ready to use.")
                
                # Main menu
                while True:
                    print("\nüìã Main Menu:")
                    print("1. Interactive Chat")
                    print("2. Load Previous Conversation")
                    print("3. Benchmark Models")
                    print("4. Compare Models")
                    print("5. Batch Processing")
                    print("6. Web UI (Gradio)")
                    print("7. Exit")
                    
                    menu_choice = input("\nEnter your choice (1-7): ").strip()
                    
                    if menu_choice == "1":
                        # Get initial prompt if user wants to provide one
                        initial_prompt = input("\nüîç Enter an initial prompt (or press Enter to skip): ").strip()
                        
                        # Options for the user
                        print("\nüìã Choose an option:")
                        print("1. Start interactive chat with default settings")
                        print("2. Configure advanced settings")
                        
                        choice = input("Enter your choice (1 or 2): ").strip()
                        
                        if choice == "2":
                            settings = advanced_settings()
                            settings["model_name"] = model_name
                            if initial_prompt:
                                settings["initial_prompt"] = initial_prompt
                            interactive_chat(**settings)
                        else:
                            interactive_chat(model_name=model_name, initial_prompt=initial_prompt)
                    
                    elif menu_choice == "2":
                        # Load previous conversation
                        chat_data = load_chat_history()
                        if chat_data:
                            print(f"\nüîÑ Resuming conversation with model: {chat_data['model']}")
                            interactive_chat(
                                model_name=chat_data['model'],
                                chat_history=chat_data['history']
                            )
                    
                    elif menu_choice == "3":
                        # Benchmark models
                        benchmark_models()
                    
                    elif menu_choice == "4":
                        # Compare models
                        compare_models()
                    
                    elif menu_choice == "5":
                        # Batch processing
                        batch_processing()
                    
                    elif menu_choice == "6":
                        # Web UI
                        create_web_ui(model_name)
                    
                    elif menu_choice == "7":
                        print("\nüëã Exiting program.")
                        !pkill ollama
                        break
                    
                    else:
                        print("\n‚ùå Invalid choice. Please try again.")
            else:
                print("‚ùå Failed to set up a model.")
        else:
            print("‚ùå Failed to set up Ollama.")
            
        # Sample usage example
        print("\nüìù Example API usage (you can copy and modify this code):")
        print("""
# Simple API call to DeepSeek model
response = query_deepseek(
    prompt="Explain quantum computing in simple terms.",
    model="deepseek-llm:7b",  # Use the model name you selected
    stream=False,
    system_prompt="You are a helpful assistant that explains complex topics simply.",
    max_tokens=1024,
    temperature=0.7
)
print(response)
        """)
    else:
        print("‚ùå This script is designed to run in Google Colab environment.")

# Display styled footer
display(HTML("""
<div style="background-color: #f8f9fa; padding: 10px; border-radius: 10px; margin-top: 15px; text-align: center;">
    <p>Enhanced DeepSeek Models with Ollama on Google Colab | Created on April 15, 2025</p>
</div>
"""))
